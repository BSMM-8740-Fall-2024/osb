---
title: "Monte Carlo Methods"
subtitle: "BSMM8740-2-R-2024F [WEEK - 9]"
author: "L.L. Odette"
footer:  "[bsmm-8740-fall-2024.github.io/osb](https://bsmm-8740-fall-2024.github.io/osb/)"
logo: "images/logo.png"
# title-slide-attributes:
#   data-background-image: images/my-DRAFT.png
#   data-background-size: contain
#   data-background-opacity: "0.40"
format: 
  revealjs: 
    chalkboard: true
    theme: slides.scss
    multiplex: true
    transition: fade
    slide-number: true
    margin: 0.05
    html-math-method: mathjax
editor: visual
menu:
  numbers: true
execute:
  freeze: auto
---

{{< include 00-setup.qmd >}}

## Recap of last week

-   Last week we introduced the fundamental problems of inference and the biases of some intuitive estimators.
-   We also built a basic understanding of the tools used to state and then satisfy causality assumptions.

## This week

-   We will get explore Monte Carlo methods as a way to integrate difficult functions, and sample from difficult probability distributions.
-   Along the way we will look at Markov Chains which both underlie sampling methods and provide a way to model the generation of data.

## Monte Carlo (MC) Methods

Monte Carlo methods are a class of *simulation-based* methods that seek to avoid complicated and/or intractable mathematical computations.

Especially those that arise from probability distributions.

## MC Methods

We can use MC methods to estimate probabilities: for a random variable $Z$ with outcomes in a set $\Omega$, with some subset $S\subset\Omega$ and event $E\equiv Z\in S$, we can compute the probability of $E$ (i.e. $\mathbb{P}(E)$) with of samples of $Z$, say $z_1,z_2,\ldots,z_M$ as

$$
\mathbb{P}(E) = \frac{1}{M}\sum_{i=1}^M 1_{z_i\in S}
$$

## MC Methods

#### Example

If $Z\sim\mathscr{N}(1,3)$ and $S=Z:0\le Z\le 3$ then

$$
\mathbb{P}(E) = \mathbb{P}(0\le Z\le 3) = \int_0^3\frac{1}{\sqrt{2\pi3}}e^{-\frac{(t-1)^2}{2*3}}dt
$$ In which case it is easier to just use R and calculate:

```{r}
#| echo: true
#| code-fold: true
pnorm(3, mean=1, sd=sqrt(3)) - pnorm(0, mean=1, sd=sqrt(3))
```

than it is to compute the integral.

## MC Methods

#### Example continued

Surprisingly, in case we don't know that `pnorm` exists, we could do this:

```{r}
#| echo: true
#| code-fold: true
#| code-summary: MC computation
# define the event
event_E_happened <- function( x ) {
  if( 0 <= x & x <= 3 ) {
    return( TRUE ) # The event happened
  } else {
    return( FALSE ) # The event DIDN'T happen
  }
}

# Now MC says that we should generate lots of copies of Z...
NMC <- 10000; # 10000 seems like "a lot".
rnorm( NMC, mean=1, sd=sqrt(3) ) |> 
  purrr::map_lgl(event_E_happened) |> 
  sum()/NMC
```

## MC Methods

Now

::: {style="font-size: x-large"}
$$
\mathbb{P}(E) = \frac{1}{M}\sum_{i=1}^M 1_{z_i\in S}
$$
:::

is the MC estimate of $\mathbb{E}[E]$, which is unbiased because for each $i$, $\mathbb{E}[1_{z_i\in S}]=\mathbb{E}[E]$, and the variance is

::: {style="font-size: x-large"}
$$
\mathrm{Var}\left({\mathbb{P}(E)}\right)=\frac{1}{M^2}\mathrm{Var}\left(\sum_{i=1}^M 1_{z_i\in S}\right)=\frac{1}{M^2}\sum_{i=1}^M \mathrm{Var}\left(1_{z_i\in S}\right)=\frac{1}{M} \mathrm{Var}\left(1_{z_i\in S}\right)
$$
:::

## MC Methods

This is true for any function of the random variable $Z$, and

$$
\mathbb{E}\left[h(Z)\right]\approx \hat{h}=\frac{1}{M}\sum_{i=1}^M h(z_i)
$$

and the variance of $h(Z)$ decreases as $1/M$ by the same reasoning.

## Random Number Generation

Monte Carlo simulation starts with random number generation, usually split into 2 stages:

-   generation of independent uniform $(0, 1)$ random variables, and
-   conversion into random variables with a particular distribution (e.g. Normal)

[**Very important**]{.underline}: never write your own generator, always use a well validated generator from a reputable source (e.g. R).

## Random Number Generation

Pseudo-random generators found in computers use a deterministic (i.e. repeatable) algorithm to generate a sequence of (apparently) random numbers on the $(0, 1)$ interval.

What defines a good random number generator (RNG) has a long period – how long it takes before the sequence repeats itself $2^{32}$ is not enough (need at least $2^{40}$). various statistical tests to measure “randomness” – well validated software will have gone through these checks.

## Random Number Generation

Recall that $\mathscr{N}(0, 1)$ Normal random variables (mean 0, variance 1) have the probability density function:

$$
p(x)=\frac{1}{2\pi}e^{-\frac{1}{2}x^2}\equiv\phi(x)
$$ and if $X\sim\mathscr{N}(0, 1)$ then its CDF is:

$$
\mathbb{P}[X\le x] = \int_{-\infty}^x\phi(x)dx
$$ \## Random Number Generation

The Box-Muller transformation method takes two independent uniform $(0, 1)$ random numbers $y_1$, $y_2$, and defines:

$$
\begin{align*}
x_{1} & =\sqrt{-2\log y_{1}}\cos(2\pi y_{2})\\
 x_2& =\sqrt{-2\log y_{1}}\sin(2\pi y_{2})
\end{align*}
$$ It can be proved that $x_1$ and $x_2$ are independent $\mathscr{N}(0, 1)$ random variables

## Random Number Generation

```{r}
#| echo: true
#| code-fold: true
#| fig-cap: "Normal y1 vs Normal y2; independent random RVs"
#| layout-ncol: 2
#| column: page
#| fig-align: center
samples <- matrix(runif(10000), ncol=2) |> data.frame() |> 
  dplyr::mutate(
    normals = 
      purrr::map2(
        X1, X2
        ,(\(x1,x2){
          data.frame(
            y1 = sqrt( -2 * log(x1) ) * cos(2 * pi * x2)
            , y2 = sqrt( -2 * log(x1) ) * sin(2 * pi * x2) 
          )
        })
      )
  ) |> 
  tidyr::unnest(normals)  
  

samples |> 
  tidyr::pivot_longer(-c(X1,X2)) |> 
  ggplot(aes(x=value, color=name, fill=name)) + 
  geom_histogram(aes(y=..density..), bins = 60, position="identity", alpha=0.3) + 
  labs(x="Value", y="Density") + theme_minimal()

samples |> 
ggplot(aes(x=y1, y=y2)) + geom_point() + coord_fixed() + theme_minimal()

```

## Random Number Generation

Your computer is only capable of producing pseudorandom numbers. These are made by running a pseudorandom number generator algorithm which is deterministic, e.g.

```{r}
set.seed(340)
rnorm(n=10)
```

```{r}
#| echo: true
set.seed(340)
rnorm(n=10)
```

Once the RNG seed is set, the “random” numbers that R generates aren’t random at all. But someone looking at these random numbers would have a very hard time distinguishing these numbers from truly random numbers. That is what “statistical randomness” means!

## Central Limit Theorem

The CLT says that for $f=\mathbb{P}(E)$ if $\sigma^2\equiv\mathrm{Var}\left(f\right)$ is finite then the error of the MC estimate

$$
e_N(f)=\bar{f}-\mathbb{E}[f]
$$

is approximately Normal in distribution for large $M$, i.e.

$$
e_N(f)\sim\sigma M^{1/2}Z
$$ where $Z\sim\mathscr{N}(0,1)$

## MC Methods

Suppose we need to compute an expectation $\mathbb{E}[g(Z)]$ for some random variable $Z$ and some function $g:\mathbb{R}\to\mathbb{R}$. Monte Carlo methods avoid doing any integration or summation and instead just generate lots of samples of $Z$, say $z_1,z_2,\ldots,z_M$ and *estimate* $\mathbb{E}[g(Z)]$ as $\frac{1}{M}\sum_{i=1}^Mg(z_i)$. The law of large numbers states that this sample mean should be close to $\mathbb{E}[g(Z)]$.

Said another way, Monte Carlo replaces the work of computing an integral (i.e., an expectation) with the work of generating lots of random variables.

## MC Methods

::: {style="font-size: x-large"}
We can use Monte Carlo to estimate probabilities of the form $\mathbb{P}\left[E\right]$ by approximating expectations of the form $\mathbb{E}[1_{X\in E}]$.

If $X\sim\mathscr{N}(\mu,\sigma)$ and we want to compute $\mathbb{E}[\log|X|]$, we could set up and solve the integral

$$
\mathbb{E} \log |X|
= \int_{-\infty}^\infty \left( \log |t| \right) f( t; \mu, \sigma) dt
= \int_{-\infty}^\infty \frac{ \log |t| }{ \sqrt{2\pi \sigma^2} }
                  \exp\left\{ \frac{ -(t-\mu)^2 }{ 2\sigma^2 } \right\}dt
$$

Alternatively, we could just draw lots of Monte Carlo replicates $X_1,X_2,\cdots,X_M$ from a normal with mean $\mu$ and variance $\sigma^2$, and look at the sample mean $M^{-1}\sum_{i=1}^M\log|x_i|$, once again appealing to the law of large numbers to ensure that this sample mean is close to its expectation.

Monte Carlo replaces the work of computing an integral (i.e., an expectation) with the work of generating lots of random variables.
:::

## MC Methods

::: {style="font-size: x-large"}
This idea can be pushed still further. Suppose that we want to compute an integral

$$
\int_Dg(x)dx
$$ where $D$ is some domain of integration and $g(.)$ is a function.

Let $f(x)$ be the density of some random variable with $f(x)>0, \forall x\in D$. In other words, $f$ is the density of a random variable supported on $D$. Then we can rewrite the integral as

$$
\int_Dg(x)dx = \int_D\frac{g(x)}{f(x)}f(x)dx = \mathbb{E}[h(x)]
$$ where $h(x)=g(x)/f(x)$ and $X\sim f$
:::

## MC Methods

::: {style="font-size: x-large"}
Now suppose we are given $h(x)$ and we want to compute $\mathbb{E}[h(X)]$ where $X\sim f,\,x\in D$. So we need to sample from $f$, but what if we could not do that directly?

If there were some other distribution $g(x)$ we could sample from, such that $g(x)>0,\,x\in D$, then

$$
\begin{align*}
\mathbb{E}_{f}\left[h(x)\right] & =\int_{D}h(x)f(x)dx\\
 & =\int_{S}h(x)\frac{f(x)}{g(x)}g(x)dx=\mathbb{E}_{g}\left[h(x)\frac{f(x)}{g(x)}\right]\\
 & =\frac{1}{n}\sum_{i=1}^{n}h(x_{i})\frac{f(x_{i})}{g(x_{i})}\quad x_{i}\sim g
\end{align*}
$$
:::

## MC Methods

::: {style="font-size: x-large"}
This is called importance sampling (IS).

1.  draw iid $x_1,x_2,\ldots,x_n$ from g and calculate the importance weight $$
    w(x_i)=\frac{f(x_{i})}{g(x_{i})}
    $$
2.  estimate $\mathbb{E}_f(h)$ by $$
    \hat{\mu}_h=\frac{1}{n}\sum_{i=1}^nw(x_i)h(x_i)
    $$
:::

## MC Methods

#### example

::: {style="font-size: x-large"}
Estimate $\mathbb E_f(X)$ where $f(x) = \sqrt{2/\pi}e^{-\frac{x^2}{2}};\;x\ge 0$ (this is the half-Normal distribution)
:::

```{r}
#| echo: true
n <- 5000
X <- rexp(n, rate=2)
W <- exp(-0.5 * X^2 + 2*X) / sqrt(2 * pi)

mu_h  <- mean(W*X)
var_h <- var(W*X)/n
se_h  <- sqrt(var_h)

tibble::tibble(mean = mu_h,  variance = var_h, 'standard error' = se_h) |> 
  gt::gt() |> 
  gt::fmt_number(decimals=4)
```

## MC Methods

#### unknown normalizing constant

::: {style="font-size: x-large"}
Suppose that $q(x)>0;\;x\in D$ and $\int_Dq(x)dx=Z_q<\infty$ The $q()$ is an un-normalized density on $D$ whereas the corresponding normalized density is $\frac{1}{Z_q}q(x)$.

Using IS, let $g(x) = \frac{1}{Z_r}r(x);\;Z_r=\int r(x)dx$, so $r$ is an un-normalized density with $Z_r$ possibly unknown.

1.  Draw $x_1,x_2,\ldots,x_n$ from $g(x)$ and calculate importance weights $w(x_i)=g(x_i)/r(x_i)$
2.  Estimate $\mathbb{E}_f\left[h(X)\right]$ by $$
    \hat{\mu_h}=\frac{\sum_{i=1}^nw(x_i)h(x_i)}{\sum_{i=1}^nw(x_i)}
    $$
:::

## MC Methods

#### unknown normalizing constant

::: {style="font-size: x-large"}
since

$$
\begin{align*}
\frac{1}{n}\sum_{i=1}^{n}w(x_{i}) & \rightarrow\mathbb{E}_{g}\left[\frac{q(X)}{r(X)}\right]=\int\frac{q(X)}{r(X)}g(x)dx=\frac{Z_{q}}{Z_{r}}\\
\frac{1}{n}\sum_{i=1}^{n}w(x_{i}) & \rightarrow\mathbb{E}_{g}\left[\frac{q(X)}{r(X)}h(X)\right]=\int\frac{q(X)}{r(X)}g(x)h(x)dx=\frac{1}{Z_{r}}\int g(x)h(x)dx
\end{align*}
$$
:::

## MC Methods

#### repeating the prior example, but un-normalized

::: {style="font-size: x-large"}
Estimate $\mathbb E_f(X)$ where $f(x) = e^{-\frac{x^2}{2}};\;x\ge 0$ (this is the half-Normal distribution, un-normalized)
:::

```{r}
#| echo: true
# un-normalized weights
n <- 5000
X <- rexp(n, rate=2)
W <- exp(-0.5 * X^2 + 2*X)

mu_h2  <- sum(W*X)/sum(W)
var_h2 <- var(W/mean(W))
se_h2  <- sqrt(var_h2)

tibble::tibble(mean = mu_h2,  variance = var_h2, 'standard error' = se_h2) |> 
  gt::gt() |> 
  gt::fmt_number(decimals=4)
```

## MC Methods

#### rejection sampling procedure

::: {style="font-size: x-large"}
Assume we have an un-normalized $g(x)$, i.e. $\pi(x)=cg(x)$ but $c$ is unknown.

We want to generate iid samples $x_1,x_2,\ldots,x_M\sim \pi$ to estimate $\mathbb{E}_\pi[h]$

Now assume we have an easily sampled density $f$, and known $K>0$, such that $Kf(x)\ge g(x),\;\forall x$, i.e. $Kf(x)\ge \pi(x)/c$ ( or $cKf(x)\ge \pi(x)$).

Then use the following procedure:

-   sample $X\sim f$ and $U\sim \mathrm{uniform}[0,1]$
-   if $U\le\frac{g(X)}{Kf(x)}$, the accept X as a draw from $\pi$
-   otherwise reject the sample and repeat
:::

## MC Methods

#### rejection sampling

::: {style="font-size: x-large"}
-   Since $0\le\frac{g(x)}{Kf(x)}\le 1$ we know that $\mathbb{P}\left(U\le\frac{g(X)}{Kf(X)}|X=x\right)=\frac{g(x)}{Kf(x)}$

-   and so

$$
\mathbb{E}_f\left[\mathbb{P}\left(U\le\frac{g(X)}{Kf(X)}|X=x\right)\right]=\mathbb{E}_f\left[\frac{g(X)}{Kf(X)}\right]=\int_{-\infty}^\infty\frac{g(X)}{Kf(X)}f(x)dx=\int_{-\infty}^\infty\frac{g(X)}{K}dx
$$
:::

## MC Methods

#### rejection sampling

::: {style="font-size: x-large"}
-   Since $0\le\frac{g(x)}{Kf(x)}\le 1$ we know that $\mathbb{P}\left(U\le\frac{g(X)}{Kf(X)}|X=x\right)=\frac{g(x)}{Kf(x)}$

-   and so

$$
\mathbb{E}_f\left[\mathbb{P}\left(U\le\frac{g(X)}{Kf(X)}|X=x\right)\right]=\mathbb{E}_f\left[\frac{g(X)}{Kf(X)}\right]=\int_{-\infty}^\infty\frac{g(X)}{Kf(X)}f(x)dx=\int_{-\infty}^\infty\frac{g(X)}{K}dx
$$
:::

## MC Methods

#### rejection sampling

::: {style="font-size: x-large"}
Similarly, for any $y\in\mathbb{R}$, we can calculate the joint probability

$$
\begin{align*}
\mathbb{P}\left(X\le y,U\le\frac{g(X)}{Kf(X)}\right) & =\mathbb{E}\left[1_{X\le y}1_{U\le\frac{g(X)}{Kf(X)}}\right]\\
 & =\mathbb{E}\left[1_{X\le y}\mathbb{P}\left(U\le\frac{g(X)}{Kf(X)}|X\right)\right]\\
 & =\mathbb{E}\left[1_{X\le y}\frac{g(X)}{Kf(X)}\right]=\int_{-\infty}^{y}\frac{g(x)}{Kf(x)}f(x)dx\\
 & =\int_{-\infty}^{y}\frac{g(x)}{K}dx
\end{align*}
$$

-   and so we have the joint probability (above - $\mathbb{P}(A,B)$), and the probability of acceptance (previous slide - $\mathbb{P}(B)$), so the probability, conditional on acceptance ($\mathbb{P}(A|B)$) is $\frac{\mathbb{P}(A,B)}{\mathbb{P}(B)}$ by Bayes rule.
:::

## MC Methods

#### rejection sampling

::: {style="font-size: x-large"}
$$
\mathbb{P}\left(X\le y|U\le\frac{g(X)}{Kf(X)}\right)=\frac{\int_{-\infty}^{y}\frac{g(x)}{K}dx}{\int_{-\infty}^\infty\frac{g(X)}{K}dx}=\int_{-\infty}^{y}\pi(x)dx
$$
:::

## MC Methods

#### rejection sampling

-   consider random variable $X$ with pdf/pmf $q(x)>0;\;x\in D$ which is difficult to sample from
-   we will sample from $q$ using a proposal pdf/pmf $f$ which we can sample from
-   if we can find a constant $K$ such that $q(x)\le Kf(x); \forall x\in D$. Alternatively $\frac{q(x)}{f(x)}\le K$
-   then there is a rejection method that returns $X\sim q$

## MC Methods

#### rejection sampling method

1.  given a proposal pdf/pmf $f$ we can sample from, and constant $K$ such that $\frac{q(x)}{f(x)}\le K; \forall x\in D$
2.  sample $Y_i\sim f$ and $U_i\sim\mathrm{U}[0,1]$
3.  for $U_i\le\frac{q(Y_i)}{Kf(Y_i)}$ return $X_i=Y_i$; otherwise return nothing and continue.

## MC Methods

#### rejection sampling: proof for discrete rv

::: {style="font-size: xx-large"}
We have $\mathbb{P}(X=x) = \sum_{i=1}^n\mathbb{P}(\mathrm{reject }\,Y)^{n-1}\mathbb{P}(\mathrm{draw }\,Y=x\,\mathrm{and\, accept})$

We also have

$$
\begin{align*}
 & \mathbb{P}(\mathrm{draw}\,Y=x\,\mathrm{and\,accept})\\
= & \mathbb{P}(\mathrm{draw}\,Y=x)\mathbb{P}(\left.\mathrm{accept}\,Y\right|Y=x)\\
= & f(x)\mathbb{P}(\left.U\le\frac{q(Y)}{Kf(Y)}\right|Y=x)\\
= & \frac{q(x)}{K}
\end{align*}
$$
:::

## MC Methods

#### rejection sampling: proof for discrete rv

::: {style="font-size: xx-large"}
The probability of rejection of a draw is

$$
\begin{align*}
\mathbb{P}(\mathrm{{reject}}\,Y) & =\sum_{x\in D}\mathbb{P}(\mathrm{{draw}}\,Y=x\,\mathrm{and\,reject\,it})\\
 & =\sum_{x\in D}f(x)\mathbb{P}(\left.U\ge\frac{q(Y)}{Kf(Y)}\right|Y=x)\\
 & =\sum_{x\in D}f(x)(1-\frac{q(x)}{Kf(x)})=1-\frac{1}{K}
\end{align*}
$$
:::

## MC Methods

#### rejection sampling: proof for discrete rv

::: {style="font-size: xx-large"}
and so[^1]

$$
\begin{align*}
\mathbb{P}(X=x) & =\sum_{n=1}^{\infty}\mathbb{P}(\mathrm{reject}\,Y)^{n-1}\mathbb{P}(\mathrm{draw}\,Y=x\,\mathrm{and\,accept})\\
 & =\sum_{n=1}^{\infty}\left(1-\frac{1}{K}\right)^{n-1}\frac{q(x)}{K}=q(x)
\end{align*}
$$
:::

[^1]: The **geometric distribution** is a discrete distribution that can be interpreted as the number of failures before the first success ($\mathbb{P}(X=k)=(1-p)^{k-1}p$, with mean $p$).

## MC Methods

#### rejection sampling: proof for continuous scalar rv

::: {style="font-size: x-large"}
Recal that we accept the proposal $Y$ whenever $(U,Y)\sim q_{U,Y}$ where $q_{U,Y}\left(u,y\right)=q(y)U_{0,1}(u)$ such that $U\le q(Y)/(Kf(Y))$

We have

$$
\begin{align*}
\mathbb{P}\left(X\le x\right) & =\mathbb{P}\left(\left.Y\le x\right|U\le q(Y)/Kf(Y))\right)\\
 & =\frac{\mathbb{P}\left(Y\le x,U\le q(Y)/Kf(Y))\right)}{\mathbb{P}\left(U\le q(Y)/Kf(Y))\right)}\\
 & =\frac{\int_{-\infty}^{x}\int_{0}^{q(y)/Kf(y)}f_{U,Y}\left(u,y\right)dudy}{\int_{-\infty}^{\infty}\int_{0}^{q(y)/Kf(y)}f_{U,Y}\left(u,y\right)dudy}\\
 & =\frac{\int_{-\infty}^{x}\int_{0}^{q(y)/Kf(y)}f\left(y\right)dudy}{\int_{-\infty}^{\infty}\int_{0}^{q(y)/Kf(y)}f\left(y\right)dudy}=\int_{-\infty}^{x}q(y)dy
\end{align*}
$$
:::

## MC Methods

#### rejection sampling: unknown normalizing constants

::: {style="font-size: x-large"}
In most practical scenarios, we know $f(x)$ and $q(x)$ only up to some normalizing constants

$$
f(x)=\bar{f}(x)/K_f\;\mathrm{and}\;q(x)=\bar{q}(x)/K_q
$$ We can still use rejection sampling since

$$
\frac{q(x)}{f(x)}\le K\;\mathrm{iff}\;\frac{\bar{q}(x)}{\bar{f}(x)}\le\hat{K}\equiv  K\frac{K_q}{K_f}
$$ In practice this means we can ignore the normalizing constants if we can find $\hat{K}$ to bound $\frac{\bar{q}(x)}{\bar{f}(x)}$
:::

## MC Methods

::: {style="font-size: x-large"}
Suppose we need to compute an expectation $\mathbb{E}[g(Z)]$ for some random variable $Z$ and some function $g:\mathbb{R}\to\mathbb{R}$. Monte Carlo methods avoid doing any integration or summation and instead just generate lots of samples of $Z$, say $z_1,z_2,\ldots,z_M$ and *estimate* $\mathbb{E}[g(Z)]$ as $\frac{1}{M}\sum_{i=1}^Mg(z_i)$. The law of large numbers states that this sample mean should be close to $\mathbb{E}[g(Z)]$.

Said another way, Monte Carlo replaces the work of computing an integral (i.e., an expectation) with the work of generating lots of random variables.
:::

## Markov Chains

::: {style="font-size: xx-large"}
Consider the following **simple random walk** on the integers $\mathbb Z$: We start at $0$, then at each time step, we go up by one with probability $p$ and down by one with probability $q = 1-p$. When $p = q = \frac12$, we're equally as likely to go up as down, and we call this the **simple symmetric random walk**.

The simple random walk is a simple but very useful model for lots of processes, like stock prices, sizes of populations, or positions of gas particles. (In many modern models, however, these have been replaced by more complicated continuous time and space models.) The simple random walk is sometimes called the "drunkard's walk", suggesting it could model a drunk person trying to stagger home.
:::

## Markov Chains

```{r}
#| layout-ncol: 2
#| fig-height: 10
set.seed(315)

rrw <- function(n, p = 1/2) {
  q <- 1 - p
  Z <- sample(c(1, -1), n, replace = TRUE, prob = c(p, q))
  X <- c(0, cumsum(Z))
}

n <- 20
p <- 2/3
plot(0:n, rrw(n, p),
  main = "Random walk for n = 20 steps, p = 2/3",
  xlab = "Time, n",
  ylab = bquote("Random walk," ~ X[n]),
  type = "b", col = "blue"
)
p <- 1/3
plot(0:n, rrw(n, p),
  main = "Random walk for n = 20 steps, p = 1/3",
  xlab = "Time, n",
  ylab = bquote("Random walk," ~ X[n]),
  type = "b", col = "blue"
)
```


## More

-   Read [Bayes Rules!](https://www.bayesrulesbook.com/)
-   Read [Think Bayes](http://allendowney.github.io/ThinkBayes2/index.html)
-   Read [Statistical Rethinking](https://github.com/rmcelreath/stat_rethinking_2022)

## Recap

-   We've had the smallest possible taste of statistical programming using Bayes theorem and sampling methods, in the context of adressing the limitations of off-the-shelf implementations of statistical methods and algorithms.
