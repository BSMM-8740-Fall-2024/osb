---
title: "Monte Carlo Methods"
subtitle: "BSMM8740-2-R-2024F [WEEK - 9]"
author: "L.L. Odette"
footer:  "[bsmm-8740-fall-2024.github.io/osb](https://bsmm-8740-fall-2024.github.io/osb/)"
logo: "images/logo.png"
# title-slide-attributes:
#   data-background-image: images/my-DRAFT.png
#   data-background-size: contain
#   data-background-opacity: "0.40"
format: 
  revealjs: 
    chalkboard: true
    theme: slides.scss
    multiplex: true
    transition: fade
    slide-number: true
    margin: 0.05
    html-math-method: mathjax
editor: visual
menu:
  numbers: true
execute:
  freeze: auto
---

{{< include 00-setup.qmd >}}

## Recap of last week

-   Last week we introduced the fundamental problems of inference and the biases of some intuitive estimators.
-   We also built a basic understanding of the tools used to state and then satisfy causality assumptions.

## This week

-   We will get explore Monte Carlo methods as a way to integrate difficult functions, and sample from difficult probability distributions.
-   Along the way we will look at Markov Chains which both underlie sampling methods and provide a way to model the generation of data.

## Monte Carlo (MC) Methods

Monte Carlo methods are a class of *simulation-based* methods that seek to avoid complicated and/or intractable mathematical computations.

Especially those that arise from probability distributions.

## MC Methods

We can use MC methods to estimate probabilities: for a random variable $Z$ with outcomes in a set $\Omega$, with some subset $S\subset\Omega$ and event $E\equiv Z\in S$, we can compute the probability of $E$ (i.e. $\mathbb{P}(E)$) with of samples of $Z$, say $z_1,z_2,\ldots,z_M$ as

$$
\mathbb{P}(E) = \frac{1}{M}\sum_{i=1}^M 1_{z_i\in S}
$$

## MC Methods

#### Example

If $Z\sim\mathscr{N}(1,3)$ and $S=Z:0\le Z\le 3$ then

$$
\mathbb{P}(E) = \mathbb{P}(0\le Z\le 3) = \int_0^3\frac{1}{\sqrt{2\pi3}}e^{-\frac{(t-1)^2}{2*3}}dt
$$ In which case it is easier to just use R and calculate:

```{r}
#| echo: true
#| code-fold: true
pnorm(3, mean=1, sd=sqrt(3)) - pnorm(0, mean=1, sd=sqrt(3))
```

than it is to compute the integral.

## MC Methods

#### Example continued

In case we don't know that `pnorm` exists we could do this:

```{r}
#| echo: true
#| code-fold: true
#| code-summary: MC computation
# define the event
event_E_happened <- function( x ) {
  if( 0 <= x & x <= 3 ) {
    return( TRUE ) # The event happened
  } else {
    return( FALSE ) # The event DIDN'T happen
  }
}

# Now MC says that we should generate lots of copies of Z...
NMC <- 10000; # 10000 seems like "a lot".
rnorm( NMC, mean=1, sd=sqrt(3) ) |> 
  purrr::map_lgl(event_E_happened) |> 
  sum()/NMC
```

## MC Methods

Now

::: {style="font-size: x-large"}
$$
\mathbb{P}(E) = \frac{1}{M}\sum_{i=1}^M 1_{z_i\in S}
$$
:::

is the MC estimate of $\mathbb{E}[E]$, which is unbiased because for each $i$, $\mathbb{E}[1_{z_i\in S}]=\mathbb{E}[E]$, and the variance is

::: {style="font-size: x-large"}
$$
\mathrm{Var}\left({\mathbb{P}(E)}\right)=\frac{1}{M^2}\mathrm{Var}\left(\sum_{i=1}^M 1_{z_i\in S}\right)=\frac{1}{M^2}\sum_{i=1}^M \mathrm{Var}\left(1_{z_i\in S}\right)=\frac{1}{M} \mathrm{Var}\left(1_{z_i\in S}\right)
$$
:::

## Random Number Generation

Monte Carlo simulation starts with random number generation, usually split into 2 stages:

-   generation of independent uniform $(0, 1)$ random variables, and
-   conversion into random variables with a particular distribution (e.g. Normal)

[**Very important**]{.underline}: never write your own generator, always use a well validated generator from a reputable source (e.g. R).

## Random Number Generation

Pseudo-random generators found in computers use a deterministic (i.e. repeatable) algorithm to generate a sequence of (apparently) random numbers on the $(0, 1)$ interval.

What defines a good random number generator (RNG) has a long period – how long it takes before the sequence repeats itself $2^{32}$ is not enough (need at least $2^{40}$). various statistical tests to measure “randomness” – well validated software will have gone through these checks.

## Random Number Generation

Recall that $\mathscr{N}(0, 1)$ Normal random variables (mean 0, variance 1) have the probability density function:

$$
p(x)=\frac{1}{2\pi}e^{-\frac{1}{2}x^2}\equiv\phi(x)
$$
and if $X\sim\mathscr{N}(0, 1)$ then its CDF is:

$$
\mathbb{P}[X\le x] = \int_{-\infty}^x\phi(x)dx
$$
## Random Number Generation

The Box-Muller transformation method takes two independent uniform $(0, 1)$ random numbers $y_1$, $y_2$, and defines:

$$
\begin{align*}
x_{1} & =\sqrt{-2\log y_{1}}\cos(2\pi y_{2})\\
 x_2& =\sqrt{-2\log y_{1}}\sin(2\pi y_{2})
\end{align*}
$$
It can be proved that $x_1$ and $x_2$ are independent $\mathscr{N}(0, 1)$ random variables

## Random Number Generation
```{r}
#| echo: true
#| code-fold: true
#| fig-cap: "Normal y1 vs Normal y2"
#| layout-ncol: 2
#| column: page
samples <- matrix(runif(10000), ncol=2) |> data.frame() |> 
  dplyr::mutate(
    normals = 
      purrr::map2(
        X1, X2
        ,(\(x1,x2){
          data.frame(
            y1 = sqrt( -2 * log(x1) ) * cos(2 * pi * x2)
            , y2 = sqrt( -2 * log(x1) ) * sin(2 * pi * x2) 
          )
        })
      )
  ) |> 
  tidyr::unnest(normals)  
  

samples |> 
  tidyr::pivot_longer(-c(X1,X2)) |> 
  ggplot(aes(x=value, color=name, fill=name)) + 
  geom_histogram(aes(y=..density..), bins = 60, position="identity", alpha=0.3) + 
  labs(x="Value", y="Density") + theme_minimal()

samples |> 
ggplot(aes(x=y1, y=y2)) + geom_point() + coord_fixed() + theme_minimal()

```


## Central Limit Theorem

The CLT says that if $\sigma^2=\mathrm{Var}\left({\mathbb{P}(E)}\right)$ is

## MC Methods

Suppose we need to compute an expectation $\mathbb{E}[g(Z)]$ for some random variable $Z$ and some function $g:\mathbb{R}\to\mathbb{R}$. Monte Carlo methods avoid doing any integration or summation and instead just generate lots of samples of $Z$, say $z_1,z_2,\ldots,z_M$ and *estimate* $\mathbb{E}[g(Z)]$ as $\frac{1}{M}\sum_{i=1}^Mg(z_i)$. The law of large numbers states that this sample mean should be close to $\mathbb{E}[g(Z)]$.

Said another way, Monte Carlo replaces the work of computing an integral (i.e., an expectation) with the work of generating lots of random variables.

## Monte Carlo Methods

Suppose we need to compute an expectation $\mathbb{E}[g(Z)]$ for some random variable $Z$ and some function $g:\mathbb{R}\to\mathbb{R}$. Monte Carlo methods avoid doing any integration or summation and instead just generate lots of samples of $Z$, say $z_1,z_2,\ldots,z_M$ and *estimate* $\mathbb{E}[g(Z)]$ as $\frac{1}{M}\sum_{i=1}^Mg(z_i)$. The law of large numbers states that this sample mean should be close to $\mathbb{E}[g(Z)]$.

Said another way, Monte Carlo replaces the work of computing an integral (i.e., an expectation) with the work of generating lots of random variables.

## More

-   Read [Bayes Rules!](https://www.bayesrulesbook.com/)
-   Read [Think Bayes](http://allendowney.github.io/ThinkBayes2/index.html)
-   Read [Statistical Rethinking](https://github.com/rmcelreath/stat_rethinking_2022)

## Recap

-   We've had the smallest possible taste of statistical programming using Bayes theorem and sampling methods, in the context of adressing the limitations of off-the-shelf implementations of statistical methods and algorithms.
