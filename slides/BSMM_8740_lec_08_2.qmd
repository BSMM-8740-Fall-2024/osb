---
title: "Causality Part 2"
subtitle: "BSMM8740-2-R-2024F [WEEK - 8]"
author: "L.L. Odette"
footer:  "[bsmm-8740-fall-2024.github.io/osb](https://bsmm-8740-fall-2024.github.io/osb/)"
logo: "images/logo.png"
# title-slide-attributes:
#   data-background-image: images/my-DRAFT.png
#   data-background-size: contain
#   data-background-opacity: "0.40"
format: 
  revealjs: 
    theme: slides.scss
    multiplex: true
    transition: fade
    slide-number: true
    margin: 0.05
editor: visual
menu:
  numbers: true
execute:
  freeze: auto
---

{{< include 00-setup.qmd >}}

## Recap of last week

-   Last week we introduced DAGs as a method to represent our causal assumptions and infer how we might estimate the causal effects of interest.
-   We worked through a method to estimate causal effects using IPW.

## This week

-   We will look at other methods used to estimate causal effects, along with methods used for special situations.

# Causal Effect Estimation

## Inverse Probability Weighting

Last week we used IPW to create a pseudopopulation where, for every confounder level, the numbers of treated and untreated were balanced.

IPW requires us to build a model to predict the treatment, depending on the confounders (assuming we have data for all the confounders).

## Inverse Probability Weighting

Repeating our prior process, first we calculate the ipw weights

```{r}
#| echo: true
#| code-fold: true
#| label: ipw model
dat_ <- causalworkshop::net_data |> dplyr::mutate(net = as.numeric(net))

propensity_model <- glm(
  net ~ income + health + temperature
  , data = dat_, family = binomial()
)

net_data_wts <- propensity_model |>
  broom::augment(newdata = dat_, type.predict = "response") |>
  # .fitted is the value predicted by the model
  # for a given observation
  dplyr::mutate(wts = propensity::wt_ate(.fitted, net))

net_data_wts |>
  lm(malaria_risk ~ net, data = _, weights = wts) |>
  broom::tidy(conf.int = TRUE)
```

## Inverse Probability Weighting

Next we use bootstrapping to generate proper confidence intervals, first by creating a function to run the generate ipw estimates for each bootstrap split:

```{r}
#| echo: true
#| code-fold: true
#| label: ipw bootstrap function

fit_ipw <- function(split, ...) {
  # get bootstrapped data sample with `rsample::analysis()`
  if("rsplit" %in% class(split)){
    .df <- rsample::analysis(split)
  }else if("data.frame" %in% class(split)){
    .df <- split
  }

  # fit propensity score model
  propensity_model <- glm(
    net ~ income + health + temperature,
    data = .df,
    family = binomial()
  )

  # calculate inverse probability weights
  .df <- propensity_model |>
    broom::augment(type.predict = "response", data = .df) |>
    dplyr::mutate(wts = propensity::wt_ate(.fitted, net))

  # fit correctly bootstrapped ipw model
  lm(malaria_risk ~ net, data = .df, weights = wts) |>
    broom::tidy()
}

```

## Inverse Probability Weighting

Next we use bootstrapping to generate proper confidence intervals:

```{r}
#| echo: true
#| code-fold: true
#| label: ipw bootstrap process
# create bootstrap samples
bootstrapped_net_data <- rsample::bootstraps(
  dat_,
  times = 1000,
  # required to calculate CIs later
  apparent = TRUE
)

# create ipw and fit each bootstrap sample
results <- bootstrapped_net_data |>
  dplyr::mutate(
    ipw_fits = purrr::map(splits, fit_ipw))
```

## Regression Adjustment

We know that we can also predict the effect of treatment by building a model to predict the outcome using a regression model for example.

```{r}
#| echo: true
#| code-fold: true
#| label: regression model
outcome_model <- glm(
  malaria_risk ~ net + income + health + temperature + insecticide_resistance,
  data = dat_
)

outcome_model |> broom::tidy(conf.int = TRUE)
```

## Regression Adjustment

And we can also bootstrap the regression adjustment estimates to get CIs, first by creating the estimation function, then generating bootstrapped estimates:

```{r}
#| echo: true
#| code-fold: true
#| label: reg bootstrap function
fit_reg <- function(split, ...) {
  # get bootstrapped data sample with `rsample::analysis()`
  if("rsplit" %in% class(split)){
    .df <- rsample::analysis(split)
  }else if("data.frame" %in% class(split)){
    .df <- split
  }

  # fit outcome model
  glm(malaria_risk ~ net + income + health + temperature + insecticide_resistance
      , data = .df
    )|>
    broom::tidy()
}

both_results <- results |>
  dplyr::mutate(
    reg_fits = purrr::map(splits, fit_reg))
```

## IPW vs Regression Adjustment

We can compare the results:

```{r}
#| echo: true
#| code-fold: true
#| label: combine reg & ipw bootstraps
both_results_dat <- both_results |>
  dplyr::mutate(
    reg_estimate = purrr::map_dbl(
      reg_fits,
      # pull the `estimate` for net for each fit
      \(.fit) .fit |>
        dplyr::filter(term == "net") |>
        dplyr::pull(estimate)
    )
    , ipw_estimate = purrr::map_dbl(
      ipw_fits,
      # pull the `estimate` for net for each fit
      \(.fit) .fit |>
        dplyr::filter(term == "net") |>
        dplyr::pull(estimate)
    )
  ) 
```

## IPW vs Regression Adjustment

We can plot the results:

```{r}
#| echo: true
#| code-fold: true
#| message: false
#| error: false
#| label: combined reg & ipw plot
dat <- both_results_dat |> 
  dplyr::select(reg_estimate, ipw_estimate) |> 
  tidyr::pivot_longer(cols=everything(), names_to = "method", values_to = "effect estimate")

dat |>
  dplyr::mutate(method=factor(method)) |>
  ggplot( bins = 50, alpha = .5
    , aes(
      `effect estimate`, after_stat(density) , colour = method, fill = method
      )
  ) +
  geom_histogram(alpha = 0.2, position = 'identity') +
  geom_density(alpha = 0.2)
```

## IPW vs Regression Adjustment

We can compare the means and the confidence intervals:

```{r}
#| echo: true
#| code-fold: true
#| message: false
#| error: false
#| label: reg & ipw CIs

dat |> 
  dplyr::group_by(method) |> 
  dplyr::summarise(
  mean = mean(`effect estimate`)
  , lower_ci = quantile(`effect estimate`, 0.025)
  , upper_ci = quantile(`effect estimate`, 0.975)
)

```

Where each mean ate estimate is within the CIs of the other ate mean estimate.

## IPW vs Regression Adjustment

There is a package (`boot`) that performs cross-validation and CI estimation at the same time:

```{r}
#| echo: true
#| code-fold: true
#| message: false
#| warning: false
#| error: false
#| label: boot package use
# bootstrap (1000 times) using the fit_reg function
boot_out_reg <- boot::boot(
  data = causalworkshop::net_data |> dplyr::mutate(net = as.numeric(net))
  , R = 1000
  , sim = "ordinary"
  , statistic =
    (\(x,y){ # x is the data, y is a vector of row numbers for the bootstrap sample
      fit_reg(x[y,]) |>
        dplyr::filter(term == "net") |>
        dplyr::pull(estimate)
    })
)
# calculate the CIs
CIs <- boot_out_reg |>
  boot::boot.ci(L = boot::empinf(boot_out_reg, index=1L, type="jack"))

tibble::tibble(CI = c("lower", "upper"), normal = CIs$normal[-1], basic = CIs$basic[-(1:3)]
               , percent = CIs$percent[-(1:3)])

```

## Doubly Robust Estimation

#### Don’t Put All your Eggs in One Basket

We’ve learned how to use linear regression and propensity score weighting to estimate $E[Y|D=1] - E[Y|D=0] | X$. But which one should we use and when?

When in doubt, just use both! Doubly Robust Estimation is a way of combining propensity score and linear regression in a way you don’t have to rely on either of them.

## Doubly Robust Estimation

The estimator is as follows.

::: {style="font-size: x-large"}
$$
\hat{ATE} = \frac{1}{N}\sum \bigg( \dfrac{D_i(Y_i - \hat{\mu_1}(X_i))}{\hat{P}(X_i)} + \hat{\mu_1}(X_i) \bigg) - \frac{1}{N}\sum \bigg( \dfrac{(1-D_i)(Y_i - \hat{\mu_0}(X_i))}{1-\hat{P}(X_i)} + \hat{\mu_0}(X_i) \bigg)
$$

where $\hat{P}(x)$ is an estimation of the propensity score (using logistic regression, for example), $\hat{\mu_1}(x)$ is an estimation of (using linear regression, for example), and is an estimation of $E[Y|X, D=1]$. As you might have already guessed, the first part of the doubly robust estimator estimates $E[Y^1]$ and the second part estimates $E[Y^0]$. Let’s examine the first part, as all the intuition will also apply to the second part by analogy.
:::

## Doubly Robust Estimation

First let's examine the code

```{r}
#| echo: true
#| code-fold: true
#| message: false
#| error: false
#| label: doubly robust estimation
D <- "net"
Y <- "malaria_risk"
X <- c('income', 'health', 'temperature')

doubly_robust <- function(df, X, D, Y){
  ps <- # propensity score
    as.formula(paste(D, " ~ ", paste(X, collapse= "+"))) |>
    stats::glm( data = df, family = binomial() ) |>
    broom::augment(type.predict = "response", data = df) |>
    dplyr::pull(.fitted)
  
  lin_frml <- formula(paste(Y, " ~ ", paste(X, collapse= "+")))
  
  idx <- df[,D] %>% dplyr::pull(1) == 0
  mu0 <- # mean response D == 0
    lm(lin_frml, data = df[idx,]) %>% 
    broom::augment(type.predict = "response", newdata = df[,X]) |>
    dplyr::pull(.fitted)
  
  idx <- df[,D] %>% dplyr::pull(1) == 1
  mu1 <- # mean response D == 1
    lm(lin_frml, data = df[idx,]) |>  
    broom::augment(type.predict = "response", newdata = df[,X]) |> 
    dplyr::pull(.fitted)
  
  # convert treatment factor to integer | recast as vectors
  d <- df[,D] %>% dplyr::pull(1) |> as.character() |> as.numeric()
  y <- df[,Y] %>% dplyr::pull(1)
  
  mean( d*(y - mu1)/ps + mu1 ) -
    mean(( 1-d)*(y - mu0)/(1-ps) + mu0 )
}

causalworkshop::net_data |> dplyr::mutate(net = as.numeric(net)) |> 
  doubly_robust(X, D, Y)
```

## Doubly Robust Estimation

Once again, we can use bootstrap to construct confidence intervals.

```{r}
#| echo: true
#| code-fold: true
#| message: false
#| error: false
#| label: dre bootstrap

all_results_dat <- both_results_dat |>
  dplyr::mutate(
    dre_estimate = 
      purrr::map_dbl(
        splits
        , (\(x){
          rsample::analysis(x) |> 
            dplyr::mutate(net = as.numeric(net)) |> 
            doubly_robust(X, D, Y)
        })
      )
  )

all_results_dat
```

## Doubly Robust Estimation

Looking at the distribution of the estimates

```{r}
#| echo: true
#| code-fold: true
#| message: false
#| error: false
#| label: dre reg ipw plot
dat <- all_results_dat |> 
  dplyr::select(reg_estimate, ipw_estimate, dre_estimate) |> 
  tidyr::pivot_longer(cols=everything(), names_to = "method", values_to = "effect estimate")

dat |>
  dplyr::mutate(method=factor(method)) |>
  ggplot( bins = 50, alpha = .5
    , aes(
      `effect estimate`, after_stat(density) , colour = method, fill = method
      )
  ) +
  geom_histogram(alpha = 0.2, position = 'identity') +
  geom_density(alpha = 0.2)
```

## Doubly Robust Estimation

Confidence intervals, including the dre:

```{r}
#| echo: true
#| code-fold: true
#| message: false
#| error: false
#| label: dre reg ipw CIs
dat |> 
  dplyr::group_by(method) |> 
  dplyr::summarise(
  mean = mean(`effect estimate`)
  , lower_ci = quantile(`effect estimate`, 0.025)
  , upper_ci = quantile(`effect estimate`, 0.975)
)
```

## Doubly Robust Estimation

::: {style="font-size: x-large"}
The doubly robust estimator is called doubly robust because it only requires one of the models, $\hat{P}(x)$ or $\hat{\mu}(x)$, to be correctly specified. To see this, take the first part that estimates $E[Y^1]$ and take a good look at it.

$$
\hat{E}[Y^1] = \frac{1}{N}\sum \bigg( \dfrac{D_i(Y_i - \hat{\mu_1}(X_i))}{\hat{P}(X_i)} + \hat{\mu_1}(X_i) \bigg)
$$

Assume that $\hat{\mu_1}(x)$ is correct. If the propensity score model is wrong, we wouldn’t need to worry. Because if $\hat{\mu_1}(x)$ is correct, then $E[D_i(Y_i - \hat{\mu_1}(X_i))]=0$. That is because the multiplication by $T_i$ selects only the treated and the residual of $\hat{\mu_1}$ on the treated have, by definition, mean zero. This causes the whole thing to reduce to $\hat{\mu_1}(X_i)$, which is correctly estimated $E[Y^1]$ by assumption. So, you see, that by being correct, $\hat{\mu_1}(X_i)$ wipes out the relevance of the propensity score model. We can apply the same reasoning to understand the estimator of $E[Y^0]$.
:::

## Doubly Robust Estimation

::: {style="font-size: x-large"}
Replace the propensity score by a random uniform variable that goes from 0.1 to 0.9 (we don’t want very small weights to blow up the propensity score variance). Since this is random, there is no way it is a good propensity score model, but the doubly robust estimator still manages to produce an estimation that is very close to when the propensity score was estimated with logistic regression.
:::

```{r}
#| echo: true
#| code-fold: true
#| message: false
#| error: false
#| label: bad ipw doubly robust
doubly_robust_bad_ipw <- function(df, X, D, Y){
  ps <- runif(dim(df)[1], 0.1, 0.9) # wrong propensity score 
  
  lin_frml <- formula(paste(Y, " ~ ", paste(X, collapse= "+")))
  
  idx <- df[,D] %>% dplyr::pull(1) == 0
  mu0 <- # mean response D == 0
    lm(lin_frml, data = df[idx,]) %>%
    broom::augment(type.predict = "response", newdata = df[,X]) |>
    dplyr::pull(.fitted)
  
  idx <- df[,D] %>% dplyr::pull(1) == 1
  mu1 <- # mean response D == 1
    lm(lin_frml, data = df[idx,]) |>
    broom::augment(type.predict = "response", newdata = df[,X]) |>
    dplyr::pull(.fitted)
  
  # convert treatment factor to integer | recast as vectors
  d <- df[,D] %>% dplyr::pull(1) |> as.character() |> as.numeric()
  y <- df[,Y] %>% dplyr::pull(1)
  
  mean( d*(y - mu1)/ps + mu1 ) -
    mean(( 1-d)*(y - mu0)/(1-ps) + mu0 )
}
```


## Doubly Robust Estimation

```{r}
#| echo: true
#| code-fold: true
#| message: false
#| error: false
#| label: bad ipw estimate plot
all_results_dat <- all_results_dat |>
  dplyr::mutate(
    bad_ipw_estimate = 
      purrr::map_dbl(
        splits
        , (\(x){
          rsample::analysis(x) |> 
            dplyr::mutate(net = as.numeric(net)) |> # not needed
            doubly_robust_bad_ipw(X, D, Y)
        })
      )
  )

dat <- all_results_dat |> 
  dplyr::select(reg_estimate, ipw_estimate, dre_estimate, bad_ipw_estimate) |> 
  tidyr::pivot_longer(cols=everything(), names_to = "method", values_to = "effect estimate")

dat |>
  dplyr::mutate(method=factor(method)) |>
  ggplot( bins = 50, alpha = .5
    , aes(
      `effect estimate`, after_stat(density) , colour = method, fill = method
      )
  ) +
  geom_histogram(alpha = 0.2, position = 'identity') +
  geom_density(alpha = 0.2)
```

## Doubly Robust Estimation
```{r}
#| echo: true
#| code-fold: true
#| message: false
#| error: false
#| label: bad ipw estimate plot CIs
dat |> 
  dplyr::group_by(method) |> 
  dplyr::summarise(
  mean = mean(`effect estimate`)
  , lower_ci = quantile(`effect estimate`, 0.025)
  , upper_ci = quantile(`effect estimate`, 0.975)
)
```

## Doubly Robust Estimation
::: {style="font-size: x-large"}
As we can see, messing up the propensity score yields slightly different ATEs, but not by much. This covers the case that the propensity model is wrong but the outcome model is correct. What about the other situation? Let’s again take a good look at the first part of the estimator, but let’s rearrange some terms

$$
\begin{align*}
\hat{E}[Y^{1}] & =\frac{1}{N}\sum\bigg(\dfrac{D_{i}(Y_{i}-\hat{\mu_{1}}(X_{i}))}{\hat{P}(X_{i})}+\hat{\mu_{1}}(X_{i})\bigg)\\
 & =\frac{1}{N}\sum\bigg(\dfrac{D_{i}Y_{i}}{\hat{P}(X_{i})}-\dfrac{D_{i}\hat{\mu_{1}}(X_{i})}{\hat{P}(X_{i})}+\hat{\mu_{1}}(X_{i})\bigg)\\
 & =\frac{1}{N}\sum\bigg(\dfrac{D_{i}Y_{i}}{\hat{P}(X_{i})}-\bigg(\dfrac{D_{i}}{\hat{P}(X_{i})}-1\bigg)\hat{\mu_{1}}(X_{i})\bigg)\\
 & =\frac{1}{N}\sum\bigg(\dfrac{D_{i}Y_{i}}{\hat{P}(X_{i})}-\bigg(\dfrac{D_{i}-\hat{P}(X_{i})}{\hat{P}(X_{i})}\bigg)\hat{\mu_{1}}(X_{i})\bigg)
\end{align*}
$$

Now, assume that the propensity score $\hat{P}(X_i)$ is correctly specified. In this case, $E[D_i - \hat{P}(X_i)]=0$, which wipes out the part dependent on $\hat{\mu_1}(X_i)$. This makes the doubly robust estimator reduce to the propensity score weighting estimator $\frac{D_iY_i}{\hat{P}(X_i)}$, which is correct by assumption. So, even if the $\hat{\mu_1}(X_i)$ is wrong, the estimator will still be correct, provided that the propensity score is correctly specified.
:::


## More

-   Read [Statistical tools for causal inference](https://chabefer.github.io/STCI/index.html)
-   Read [Causal inference in R](https://www.r-causal.org/)

## Recap

-   We introduced the fundamental problems of inference and the biases of some intuitive estimators,
-   We developed a basic understanding of the tools used to state and then satisfy causality assumptions,
-   We looked at an example of how econometric methods recover treatment effects.
