---
title: "More tidymodels packages"
subtitle: "BSMM8740-2-R-2023F [WEEK - 5]"
author: "L.L. Odette"
footer:  "[bsmm-8740-fall-2023.github.io/osb](https://bsmm-8740-fall-2023.github.io/osb/)"
logo: "images/logo.png"
title-slide-attributes:
  data-background-image: images/my-DRAFT.png
  data-background-size: contain
  data-background-opacity: "0.40"
format: 
  revealjs: 
    theme: slides.scss
    multiplex: true
    transition: fade
    slide-number: true
    margin: 0.05
editor: visual
menu:
  numbers: true
execute:
  freeze: auto
---

```{r opts, include = FALSE}
options(width = 95)
library(knitr)
opts_chunk$set(comment="", 
               digits = 3, 
               tidy = FALSE, 
               prompt = TRUE,
               fig.align = 'center')
require(magrittr)
require(ggplot2)
theme_set(theme_bw(base_size = 18) + theme(legend.position = "top"))
```

## Recap of last week

-   Last week we looked at several regression models that are useful for predictions

-   However, these models have different conventions on how they accept data and describe the model

-   Today we look at the tidymodels package which will give us a workflow to describe, fit and compare models, using the same approach across methods.

# Tidymodels

## Tidymodels

-   Tidymodels is an R package that provides a unified and consistent framework for modeling and machine learning tasks.
-   It is built on top of the tidyverse, making it easy to integrate with other tidyverse packages.
-   Tidymodels promotes best practices, repeatability, and clear documentation in your data analysis and modeling workflow.

## Tidymodels

### Key Components of Tidymodels

-   Model Building: tidymodels provides various modeling engines for different algorithms like lm(), glm(), randomForest(), xgboost(), etc.
-   Preprocessing: Easy and flexible data preprocessing using recipes, allowing for seamless data transformation and feature engineering.

## Tidymodels

### Key Components of Tidymodels

-   Resampling: Efficient methods for handling data splitting, cross-validation, bootstrapping, and more.
-   Metrics: A wide range of evaluation metrics to assess model performance and choose the best model.

## Fitting with parsnip

We've seen how the form of the arguments to linear models in R can be very different.

Parsnip is one of the tidymodels packages that provides a standardized interface across models

We look at how to fit and predict with parsnip in the next few slides, once the data has been prepped.

## Fitting with parsnip

We can fit a linear regression with OLS (model spec) or penalized regression (x,y spec) using data structured specifically for the model.

By contrast the tidymodels approach is more uniform.

## Fitting with parsnip

1.  *Specify the type of model based on its mathematical structure* (e.g., linear regression, random forest, KNN, etc).
2.  *Specify the engine for fitting the model.* Most often this reflects the software package that should be used, like lm or **glmnet**.
3.  *When required, declare the mode of the model.* The mode reflects the type of prediction outcome. For numeric outcomes, the mode is regression; for qualitative outcomes, it is classification.

## Fitting with parsnip

The specification are built without referencing the data:

```{r}
#| echo: true
#| message: false
#| code-fold: true
#| code-summary: "tidymodel specification"
#| code-line-numbers: "2|4"
# basic linear model
parsnip::linear_reg() %>% parsnip::set_engine("lm")
# basic penalized linear model
parsnip::linear_reg() %>% parsnip::set_engine("glmnet")
```

## Fitting with parsnip

The translate function can be used to see how the spec is converted to the correct syntax for different methods

```{r}
#| echo: true
#| message: false
#| code-fold: true
#| code-summary: "tidymodel spec translation"
#| code-line-numbers: "2|4"
# basic linear model
parsnip::linear_reg() %>% parsnip::set_engine("lm") %>% parsnip::translate()
# basic penalized linear model
parsnip::linear_reg(penalty = 1) %>% parsnip::set_engine("glmnet") %>% parsnip::translate()
```

## Fitting with parsnip

The translate function can be used to see how the spec is converted to the correct syntax

```{r}
#| echo: true
#| message: false
#| eval: false
#| code-line-numbers: "2-4|6|8-10|12-16"
# prep data
data_split <- rsample::initial_split(modeldata::ames, strata = "Sale_Price")
ames_train <- rsample::training(data_split)
ames_test  <- rsample::testing(data_split)
# spec model
lm_model <- parsnip::linear_reg() %>% parsnip::set_engine("lm")
# fit model
lm_form_fit <- lm_model %>% 
  # Recall that Sale_Price has been pre-logged
   parsnip::fit(Sale_Price ~ Longitude + Latitude, data = ames_train)
# fit model with data in (x,y) form
lm_xy_fit <- 
  lm_model %>% parsnip::fit_xy(
    x = ames_train %>% dplyr::select(Longitude, Latitude),
    y = ames_train %>% dplyr::pull(Sale_Price)
  )
```

## Fitting with parsnip

Model results can be extracted from the fit object

```{r}
#| echo: true
#| message: false
#| eval: false
#| code-line-numbers: "2|4"

lm_form_fit %>% parsnip::extract_fit_engine()

lm_form_fit %>% parsnip::extract_fit_engine() %>% stats::vcov()
```

## Fitting with parsnip

Exercise: random forest regression

A list of all parsnip-type models can be found [here](https://www.tidymodels.org/find/parsnip/).

## Tidymodels

If we use the term **model** to reference a a structural equation that relates some predictors to one or more outcomes, then everything before the model is fit is related to structuring the predictors and outcomes.

The model workflow refers to the broader process, including any pre-processing steps, the model fit itself, as well as potential post-processing activities. Similar collections of steps are sometimes called pipelines.

## Tidymodels

![](/images/proper-workflow.svg)

## Tidymodels basics

```{r}
#| echo: true
#| message: false
#| eval: false
#| code-line-numbers: "2|4-9|11-13|15-18"
# create test/train splits
ames <- modeldata::ames %>% dplyr::mutate( Sale_Price = log10(Sale_Price) )

set.seed(502)
ames_split <- rsample::initial_split(
  ames, prop = 0.80, strata = "Sale_Price"
  )
ames_train <- rsample::training(ames_split)
ames_test  <- rsample::testing(ames_split)

# Create a linear regression model
lm_model <- parsnip::linear_reg() %>% 
  parsnip::set_engine("lm") 

# Create a workflow: adding a parsnip model
lm_wflow <- 
  workflows::workflow() %>% 
  workflows::add_model(lm_model)
```

## Tidymodels basics

```{r}
#| echo: true
#| message: false
#| eval: false
#| code-line-numbers: "2-4|6-7|9-10"

# preprocessing not specified; a formula is sufficient
lm_wflow %<>% 
  workflows::add_formula(Sale_Price ~ Longitude + Latitude)

# fit the model
lm_fit <- lm_wflow %>% parsnip::fit(ames_train)

# predict on the fitted workflow
lm_fit %>% stats::predict(ames_test %>% dplyr::slice(1:3))

```

## Tidymodels basics

In base R, the **predict** function returns results in a format that depends on the models.

By contrast, **parsnip** and **workflows** conforms to the following rules:

1.  The results are always a tibble.
2.  The column names of the tibble are always predictable.
3.  There are always as many rows in the tibble as there are in the input data set, and in the same order.

## Tidymodels basics

The predictable column names are

<div>

```{r}
#| echo: false
#| output: asis
#| message: false
tibble::tibble(
  'type value' = c('numeric','class','prob','conf_int','pred_int')
  , 'column name(s)' = c('.pred','.pred_class','.pred_{class levels}','.pred_lower, .pred_upper'
                         , '.pred_lower, .pred_upper')
) %>% 
  gt::gt() %>% 
  gtExtras::gt_theme_espn() %>% 
  gt::tab_options( table.font.size = gt::px(38) ) %>% 
  gt::as_raw_html()
```

</div>

## Tidymodels basics

The model and preprocessor can be removed or updated:

```{r}
#| echo: true
#| message: false
#| eval: false
#| code-line-numbers: "2-3|5-10"

# update the formula
lm_fit %>% workflows::update_formula(Sale_Price ~ Longitude)

# remove the formula and use add_variables instead
lm_wflow %<>% 
  workflows::remove_formula() %>% 
  workflows::add_variables(
    outcome = Sale_Price, predictors = c(Longitude, Latitude)
  )

```

Predictors can be selected using **tidyselect** selectors, e.g. *everything()*, *ends_with()*, etc.

# Workflowsets

## Tidymodels: workflowsets

The `workflowset`package creates combinations of workflow components. A list of preprocessors (e.g., formulas, `dplyr` selectors, or feature engineering recipe objects) can be combined with a list of model specifications, resulting in a set of workflows.

## Tidymodels: workflowsets

Create a set of preprocessors by formula

```{r}
#| echo: true
#| message: false
#| eval: false
#| code-line-numbers: "1-7|9-13|15-18"
# set up a list of formulas
location <- list(
  longitude = Sale_Price ~ Longitude,
  latitude = Sale_Price ~ Latitude,
  coords = Sale_Price ~ Longitude + Latitude,
  neighborhood = Sale_Price ~ Neighborhood
)

# create a workflowset
location_models <- 
  workflowsets::workflow_set(
    preproc = location, models = list(lm = lm_model)
  )

# view
location_models$info[[1]]
# extract
workflowsets::extract_workflow(location_models, id = "coords_lm")
```

## Tidymodels: workflowsets

Create model fits

```{r}
#| echo: true
#| message: false
#| eval: false
#| code-line-numbers: "1-8|10-11"
# create a new column (fit) by mapping fit against the data in the info column
location_models %<>%
   dplyr::mutate(
     fit = purrr::map(
       info
       , ~ parsnip::fit(.x$workflow[[1]], ames_train)
      )
   )

# view
location_models$fit[[1]]
```

## Tidymodels: workflowsets

There is a convenience function called `last_fit()` that will *fit* the model to the entire training set and *evaluate* it with the testing set.

```{r}
#| echo: true
#| message: false
#| eval: false
#| code-line-numbers: "2|4-7"

final_lm_res <- tune::last_fit(lm_wflow, ames_split)

# view
final_lm_res %>% dplyr::glimpse()
# pull the fitted workflow
fitted_lm_wflow <-  workflowsets::extract_workflow(final_lm_res)
```

## Tidymodels: workflowsets

Similarly, **collect_metrics()** and **collect_predictions()** provide access to the performance metrics and predictions, respectively.

```{r}
#| echo: true
#| message: false
#| eval: false
#| code-line-numbers: "2-3|5-6"

# get the fit metrics
workflowsets::collect_metrics(final_lm_res)

# get the predictions, post-fit
workflowsets::collect_predictions(final_lm_res) %>% dplyr::slice(1:5)
```

## Tidymodels: workflowsets

When using **predict(workflow, new_data)**, no model or preprocessor parameters like those from recipes are re-estimated using the values in new_data. Take centering and scaling using **step_normalize()** as an example. Using this step, the means and standard deviations from the appropriate columns are determined from the training set; new samples at prediction time are standardized using these values from training when **predict()** is invoked.

## Tidymodels: workflowsets

```{r}
#| echo: true
#| message: false
#| eval: false

lm_fit %>%
  # pull the parsnip object
  workflows::extract_fit_parsnip() %>% 
  # tidy up the fit results
  yardstick::tidy() %>% 
  # show the first n rows
  dplyr::slice_head(n=5)
```

# Yardstick

## Tidymodels: yardstick

### Performance metrics and inference

An inferential model is used primarily to understand relationships, and typically emphasizes the choice (and validity) of probabilistic distributions and other generative qualities that define the model.

For a model used primarily for prediction, by contrast, predictive strength is of primary importance and other concerns about underlying statistical qualities may be less important.

# Yardstick

## Tidymodels: yardstick

```{r}
#| echo: false
#| message: false
#| eval: false
#| code-fold: true
#| code-line-numbers: "1-6|8-17|19-26"
ames <- dplyr::mutate(modeldata::ames, Sale_Price = log10(Sale_Price))

set.seed(502)
ames_split <- rsample::initial_split(ames, prop = 0.80, strata = Sale_Price)
ames_train <- rsample::training(ames_split)
ames_test  <- rsample::testing(ames_split)

ames_rec <- 
  recipes::recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + 
           Latitude + Longitude, data = ames_train) %>%
  recipes::step_log(Gr_Liv_Area, base = 10) %>% 
  recipes::step_other(Neighborhood, threshold = 0.01) %>% 
  recipes::step_dummy(
    recipes::all_nominal_predictors()
  ) %>% 
  recipes::step_interact( ~ Gr_Liv_Area:starts_with("Bldg_Type_") ) %>% 
  recipes::step_ns(Latitude, Longitude, deg_free = 20)
  
lm_model <- parsnip::linear_reg() %>% set_engine("lm")

lm_wflow <- 
  workflows::workflow() %>% 
  workflows::add_model(lm_model) %>% 
  workflows::add_recipe(ames_rec)

lm_fit <- parsnip::fit(lm_wflow, ames_train)
```

## Tidymodels: yardstick

### Regression metrics

```{r}
#| echo: true
#| message: false
#| eval: false
#| code-line-numbers: "2-7|9-13|15"
# fit with new data
ames_test_res <- 
  stats::predict(
    lm_fit
    , new_data = ames_test %>% dplyr::select(-Sale_Price)
  )
ames_test_res

# compare predictions with corresponding data
ames_test_res <- 
  dplyr::bind_cols(
    ames_test_res, ames_test %>% dplyr::select(Sale_Price))
ames_test_res

yardstick::rmse(ames_test_res, truth = Sale_Price, estimate = .pred)
```

## Tidymodels: yardstick

### Regression metrics: numeric targets

```{r}
#| echo: true
#| message: false
#| eval: false
#| code-line-numbers: "1-7|9"
# create metric set
ames_metrics <- 
  yardstick::metric_set(
    yardstick::rmse
    , yardstick::rsq
    , yardstick::mae
  )

ames_metrics(ames_test_res, truth = Sale_Price, estimate = .pred)

```

## Tidymodels: yardstick

### Classification metrics: binary class targets

```{r}
#| echo: true
#| message: false
#| eval: false
#| code-line-numbers: ""
# data for two-set classification
two_class_example <- modeldata::two_class_example

tibble::tibble(two_class_example)

# compute the confusion matrix: 
yardstick::conf_mat(two_class_example, truth = truth, estimate = predicted)

# compute the accuracy:
yardstick::accuracy(two_class_example, truth, predicted)

# Matthews correlation coefficient:
yardstick::mcc(two_class_example, truth, predicted)

```

## Tidymodels: yardstick

### Classification metrics: binary class targets

```{r}
#| echo: true
#| message: false
#| eval: false
#| code-line-numbers: "1-2|3-10|12-16"
# F1 metric:
yardstick::f_meas(two_class_example, truth, predicted)

# Combining these three classification metrics together
classification_metrics <- 
  yardstick::metric_set(
    yardstick::accuracy
    , yardstick::mcc
    , yardstick::f_meas
  )

classification_metrics(
  two_class_example
  , truth = truth
  , estimate = predicted
)
```

## Tidymodels: yardstick

### Classification metrics: class probabilities

```{r}
#| echo: true
#| message: false
#| eval: false
#| code-line-numbers: "1-6|8|10|12"
two_class_curve <- 
  yardstick::roc_curve(
    two_class_example
    , truth
    , Class1
  )

two_class_curve

yardstick::roc_auc(two_class_example, truth, Class1)

parsnip::autoplot(two_class_curve)
```

There are other functions that use probability estimates, including `gain_curve`, `lift_curve`, and pr_curve.

## Tidymodels: yardstick

### Regression metrics: multi-class targets

-   The functions for metrics that use the discrete class predictions are identical to their binary counterparts.
-   Metrics designed to handle outcomes with only two classes are extended for outcomes with more than two classes.

## Tidymodels: yardstick

### Regression metrics: multi-class targets

Take sensitivity for example:

::: {style="font-size: smaller"}
-   Macro-averaging computes a set of one-versus-all metrics using the standard two-class statistics. These are averaged.
-   Macro-weighted averaging does the same but the average is weighted by the number of samples in each class.
-   Micro-averaging computes the contribution for each class, aggregates them, then computes a single metric from the aggregates.
:::

## Tidymodels: yardstick

### Regression metrics: multi-class targets

```{r}
#| echo: true
#| message: false
#| eval: false
#| code-line-numbers: "1-6|8-11"
# load the data and convert it to a tibble
hpc_cv <- modeldata::hpc_cv %>% tibble::tibble()
# compute accuracy (same as binary case)
yardstick::accuracy(hpc_cv, obs, pred)
# compute matthews correlation coefficient (same as binary case)
yardstick::mcc(hpc_cv, obs, pred)

# apply the sensitivity metrics
yardstick::sensitivity(hpc_cv, obs, pred, estimator = "macro")
yardstick::sensitivity(hpc_cv, obs, pred, estimator = "macro_weighted")
yardstick::sensitivity(hpc_cv, obs, pred, estimator = "micro")
```

## Tidymodels: yardstick

### Regression metrics: multi-class targets

```{r}
#| echo: true
#| message: false
#| eval: false
#| code-line-numbers: "1-4|7-9|11-14"
# multi-class estimates for probability metrics
hpc_cv %>% yardstick::roc_auc(obs, VF, F, M, L)
# multi-class estimates with estimator (one of "hand_till", "macro", or "macro_weighted")
hpc_cv %>% yardstick::roc_auc(obs, VF, F, M, L, estimator = "macro_weighted")

# show metrics by groups (re-sampling in this case)
hpc_cv %>% 
  dplyr::group_by(Resample) %>% 
  yardstick::accuracy(obs, pred)

hpc_cv %>% 
  dplyr::group_by(Resample) %>% 
  yardstick::roc_curve(obs, VF, F, M, L) %>% 
  autoplot()
```

## Tidymodels: performance evaluation

**Re-substitution**: comparison using same training data

```{r}
#| echo: true
#| message: false
#| eval: false
#| code-line-numbers: "1-5|7-13|15-16"
# create  random forest model object
rf_model <- 
  parsnip::rand_forest(trees = 1000) %>% 
  parsnip::set_engine("ranger") %>% 
  parsnip::set_mode("regression")

# create a workflow using the random forest model
rf_wflow <- 
  workflows::workflow() %>% 
  workflows::add_formula(
    Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + 
      Latitude + Longitude) %>% 
  workflows::add_model(rf_model) 

# fit the random forest model with the ames training set
rf_fit <- rf_wflow %>% parsnip::fit(data = ames_train)
```

## Tidymodels: performance evaluation

A function to compare models

```{r}
#| echo: true
#| message: false
#| eval: false
#| code-line-numbers: "3-6|8-10|12-18"
estimate_perf <- function(model, dat) {
  # Capture the names of the `model` and `dat` objects
  cl <- match.call()
  obj_name <- as.character(cl$model)         # get the model name
  data_name <- as.character(cl$dat)          # get the dataset name
  data_name <- gsub("ames_", "", data_name)  # replace underlines
  
  # Estimate these metrics:
  reg_metrics <- 
    yardstick::metric_set(yardstick::rmse, yardstick::rsq)
  
  model %>%
    stats::predict(dat) %>%                          # predict
    dplyr::bind_cols(dat %>% dplyr::select(Sale_Price)) %>% 
    reg_metrics(Sale_Price, .pred) %>%
    dplyr::select(-.estimator) %>%
    dplyr::mutate(object = obj_name, data = data_name)
}
```

## Tidymodels: performance evaluation

Use the function on the random forest and linear models

```{r}
#| echo: true
#| message: false
#| eval: false
#| code-line-numbers: "1-2|4-5|6-7"
# get performance of the random forest model (train)
estimate_perf(rf_fit, ames_train)

# get performance of the linear model (train)
estimate_perf(lm_fit, ames_train)

# get performance of the linear model  (test)
estimate_perf(rf_fit, ames_test)
```

## Tidymodels: performance evaluation

Summarize and present the performance comparison

```{r}
#| echo: true
#| message: false
#| eval: false
#| code-fold: true
#| code-line-numbers: "3-6|8-9|10-12|"
# get performance of the random forest model (train)
dplyr::bind_rows(
  estimate_perf(rf_fit, ames_train)
  , estimate_perf(lm_fit, ames_train)
  , estimate_perf(rf_fit, ames_test)
  , estimate_perf(lm_fit, ames_test)
) %>% 
  dplyr::filter(.metric == 'rmse') %>% 
  dplyr::select(-.metric) %>% 
  tidyr::pivot_wider(
    names_from = data
    , values_from = .estimate
  ) %>% 
  gt::gt() %>% 
  gt::fmt_number(decimals=4) %>% 
  gt::tab_header(title = "Performance statistics") %>% 
  gtExtras::gt_theme_espn()

```

## Tidymodels: performance evaluation

**Re-sampling**: iterative comparison:

![](/images/resampling.svg){fig-align="center" width="494"}

## Tidymodels: performance evaluation

**Re-sampling**: V-fold cross validation:

*In V*-fold cross-validation. The data are randomly partitioned into *V* sets of roughly equal size (called the folds).

Larger *V* values result in resampling estimates with small bias but substantial variance. Smaller values of *V* have large bias but low variance. 

```{r}
#| echo: true
#| message: false
#| eval: false
#| code-line-numbers: "1-3-6|4-5"
set.seed(1001)
ames_folds <- rsample::vfold_cv(ames_train, v = 10)
ames_folds
# rsample::analysis() and rsample::assessment() extract samples
ames_folds$splits[[1]] %>% rsample::analysis() %>% dim()
```

## Tidymodels: performance evaluation

**Re-sampling**: cross validation variants: repeated cross validation

R repetitions of V-fold cross-validation reduces variance by a factor of $1/\sqrt{\text{R}}$.

```{r}
#| echo: true
#| message: false
#| eval: false
rsample::vfold_cv(ames_train, v = 10, repeats = 5)
```

## Tidymodels: performance evaluation

**Re-sampling**: cross validation variants: LOO and MCCV

-   Leave-one-out (LOO) cross-validation with a training set of $n$ rows generates fit using $n-1$ rows of the training set (`rsample::loo_cv`).
-   Monte carlo cross validation (MCCV) uses a fixed proportion of the training set, randomly selected on each cross validation, making the assessment sets not mutually exclusive (e.g. (`rsample::mc_cv( prop=0.9, times=20 )`).

## Tidymodels: performance evaluation

**Re-sampling**: validation sets:

Validation sets are often used when the original pool of data is very large. In this case, a single large partition may be adequate to characterize model performance without having to do multiple resampling iterations.

With the **rsample** package, a validation set is like any other resampling object; this type is different only in that it has a single iteration.

## Tidymodels: performance evaluation

**Re-sampling**: bootstrapping:

A bootstrap sample of the training set is a sample that is the same size as the training set but is drawn *with replacement.*

When bootstrapping, the assessment set is often called the *out-of-bag* sample.

```{r}
#| echo: true
#| message: false
#| eval: false
rsample::bootstraps(ames_train, times = 5)
```

## Tidymodels: performance evaluation

**Re-sampling**: resampling time:

When the data have a strong time component, a resampling method needs to support modeling to estimate seasonal and other temporal trends within the data. 

For this type of resampling, the size of the initial analysis and assessment sets are specified and subsequent iterations are shifted in time

## Tidymodels: performance evaluation

**Re-sampling**: resampling time:

![](/images/rolling.svg){fig-align="center"}

## Tidymodels: performance evaluation

**Re-sampling**: resampling time:

Two different configurations of this method:

-   The analysis set can cumulatively grow (as opposed to remaining the same size). After the first initial analysis set, new samples can accrue without discarding the earlier data.
-   The resamples need not increment by one. For example, for large data sets, the incremental block could be a week or month instead of a day.

## Tidymodels: performance evaluation

**Re-sampling**: resampling time:

For a year's worth of data, suppose that six sets of 30-day blocks define the analysis set. For assessment sets of 30 days with a 29-day skip, we can use the **rsample** package to specify:

```{r}
#| echo: true
#| message: false
#| eval: false
#| code-line-numbers: "2|3"
time_slices <- 
  tibble::tibble(x = 1:365) %>% 
  rsample::rolling_origin(initial = 6 * 30, assess = 30, skip = 29, cumulative = FALSE)
```

## Tidymodels: performance evaluation

**Re-sampling**: resampling time:

```{r}
#| echo: true
#| message: false
#| eval: false
#| code-line-numbers: "2|3-7|9-13"
# pull out first and last data points in the analysis dataset
time_slices$splits %>% 
  purrr::map_dfr( 
    .f = 
      ~rsample::analysis(.x) %>% 
      dplyr::summarize(first = min(.), last = max(.))
  )
# pull out first and last data points in the assessment dataset
time_slices$splits %>% 
  purrr::map_dfr(
    .f = ~rsample::assessment(.x) %>% 
      dplyr::summarize(first = min(.), last = max(.))
  )

```

## Tidymodels: performance evaluation

**Evaluation**:

1.  During resampling, the analysis set is used to preprocess the data, apply the pre-processing to itself, and use these processed data to fit the model.
2.  The pre-processing statistics produced by the analysis set are applied to the assessment set. The predictions from the assessment set estimate performance on new data.

This sequence repeats for every resample.

## Tidymodels: performance evaluation

**Evaluation**:

The function **tune::fit_resamples** is like parsnip::fit with a resamples argment instead of a data argument:

```{r}
#| echo: true
#| message: false
#| eval: false
#| code-line-numbers: "2|4|6"

model_spec %>% fit_resamples(formula,  resamples, ...)

model_spec %>% fit_resamples(recipe,   resamples, ...)

workflow   %>% fit_resamples(          resamples, ...)
```

## Tidymodels: performance evaluation

**Evaluation**:

Optional arguments are:

-   **metrics**: A metric set of performance statistics to compute. By default, regression models use RMSE and R^2^ while classification models compute the area under the ROC curve and overall accuracy.

-   **control**: A list created by **tune::control_resamples()** with various options.

## Tidymodels: performance evaluation

**Evaluation**:

Control arguments are:

-   **`verbose`**: A logical for printing logging.
-   **`extract`**: A function for retaining objects from each model iteration (discussed later).
-   **`save_pred`**: A logical for saving the assessment set predictions.

## Tidymodels: performance evaluation

Save the predictions in order to visualize the model fit and residuals:

```{r}
#| echo: true
#| message: false
#| eval: false
#| code-line-numbers: "1|3-6"
keep_pred <- tune::control_resamples(save_pred = TRUE, save_workflow = TRUE)

set.seed(1003)
rf_res <- 
  rf_wflow %>% 
  tune::fit_resamples(resamples = ames_folds, control = keep_pred)
rf_res
```

## Tidymodels: performance evaluation

The return value is a tibble similar to the input resamples, along with some extra columns:

-   `.metrics` is a list column of tibbles containing the assessment set performance statistics.
-   `.notes` is list column of tibbles cataloging any warnings/errors generated during resampling.
-   `.predictions` is present when `save_pred = TRUE`. This list column contains tibbles with the out-of-sample predictions.

## Tidymodels: performance evaluation

Convenience functions in tidymodels can extract and format the results produced by tuning functions, e.g.

```{r}
#| echo: true
#| message: false
#| eval: false
#| code-line-numbers: "1|3|5"
rf_res %>% tune::collect_metrics()

rf_res %>% tune::collect_predictions()

val_res %>% tune::collect_metrics()
```

## Tidymodels: parallelism

The models created during resampling are independent of one another and can be processed in parallel across processors on the same computer.

```{r}
#| echo: true
#| message: false
#| eval: false
#| code-line-numbers: "1-2|4-6"
# The number of physical cores in the hardware:
parallel::detectCores(logical = FALSE)

# The number of possible independent processes that can 
# be simultaneously used:  
parallel::detectCores(logical = TRUE)

```

## Recap

-   In this section we have worked with the `tidymodels` package to build a workflow that facilitates building and evaluating multiple models.

-   Combined with the recipes package we now have a complete data modeling framework.
