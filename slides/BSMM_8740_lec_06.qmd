---
title: "Classification & clustering methods"
subtitle: "BSMM8740-2-R-2023F [WEEK - 6]"
author: "L.L. Odette"
footer:  "[bsmm-8740-fall-2023.github.io/osb](https://bsmm-8740-fall-2023.github.io/osb/)"
logo: "images/logo.png"
# title-slide-attributes:
#   data-background-image: images/my-DRAFT.png
#   data-background-size: contain
#   data-background-opacity: "0.40"
format: 
  revealjs: 
    theme: slides.scss
    multiplex: true
    transition: fade
    slide-number: true
    margin: 0.05
editor: visual
menu:
  numbers: true
execute:
  freeze: auto
---

```{r opts, include = FALSE}
options(width = 95)
library(knitr)
opts_chunk$set(comment="", 
               digits = 3, 
               tidy = FALSE, 
               prompt = TRUE,
               fig.align = 'center')
require(magrittr)
require(ggplot2)
theme_set(theme_bw(base_size = 18) + theme(legend.position = "top"))
```

## Recap of last week

-   Last time we introduced the Tidymodels framework in R

-   We showed how we can use the Tidymodels framework to create a workflow for data prep, feature engineering, model fitting and model evaluation.

-   Today we look at the using the Tidymodels package to build classification and clustering models.

# Classification Methods

## Classification

-   Classification is a supervised machine learning method where the model tries to predict the correct class label for a given input data.
-   In classification, the model is fully trained using the training data, and then it is evaluated on test data before being used to perform prediction on new unseen data.

## Classification

**Eager learners** are machine learning algorithms that first build a model from the training dataset before making any prediction on future datasets. They spend more time on the training process to better generalize from the data.

They usually require less time to make predictions. Example eager learners are:

-   Logistic Regression.
-   Support Vector Machine.
-   Decision Trees.
-   Artificial Neural Networks.

## Classification

**Lazy learners or instance-based learners**, do not create any model immediately from the training data, and this where the lazy aspect comes from. They just memorize the training data, and each time there is a need to make a prediction, they search for the nearest neighbor from the whole training data. Examples are:

-   K-Nearest Neighbor.
-   Case-based reasoning.

## Types of classification

-   Binary classification
-   Multi-Class Classification (mutually exclusive)
    -   [multiclass](https://juliasilge.com/blog/datasaurus-multiclass/)
-   Multi-Label Classification (not mutually exclusive)
    -   [multilabel](https://en.wikipedia.org/wiki/Multi-label_classification)
-   Imbalanced Classification
    -   [class imbalance](https://www.tidymodels.org/learn/models/sub-sampling/)

## Binary Logistic Regression

Logistic regression is a Generalized Linear Model where the dependent (categorical) variable $y$ takes values in $\{0,1\}$. This can be interpreted as identifying two classes, and logistic regression provides a prediction for class membership based on a linear combination of the explanatory variables.

Logistic regression is an example of supervised learning.

## Binary Logistic Regression

For the logistic GLM:

-   the distribution of the observations is Binomial with parameter $\pi$
-   the explanatory variables are linear in the parameters: $\eta=\beta_0+\beta_1 x_1+\beta_2 x_2+\beta_2 x_2\ldots+\beta_n x_n$
-   the link function is the logit: $\eta=\text{logit}(\pi) = \log(\frac{\pi}{1-\pi})$

It follows that $\pi = \frac{e^\eta}{1+e^\eta} = \frac{1}{1+e^{-\eta}}$, which is a sigmoid function in the explanatory variables. The equation $\eta=0$ defines a linear decision boundary or classification threshold.

## Binary Logistic Regression

::: columns
::: {.column width="40%"}
```{r}
tibble::tibble(eta = seq(-5,5,0.2)) %>% dplyr::mutate(pi = 1/(1+exp(-eta))) %>% 
ggplot(aes(x=eta, y=pi)) + geom_point()
```
:::

::: {.column width="60%" style="font-size: smaller"}
$$\begin{align*}
\pi & =\frac{1}{1+e^{-\eta}}\;\text{(logistic function)}\\
\log\left(\frac{\pi}{1-\pi}\right) & =\log\left(\frac{\frac{1}{1+e^{-\eta}}}{1-\frac{1}{1+e^{-\eta}}}\right)\\
 & =\log\left(\frac{\frac{1}{1+e^{-\eta}}}{\frac{e^{-\eta}}{1+e^{-\eta}}}\right)=\log\left(e^{\eta}\right)=\eta
\end{align*}$$
:::
:::

## Binary Logistic Regression

The term $\frac{\pi}{1-\pi}$ is called the the odds-ratio. By its definition $\frac{\pi}{1-\pi}=e^{\beta_0+\beta_1 x_1+\beta_2 x_2+\beta_2 x_2\ldots+\beta_n x_n}$

So if $x_1$ changes by one unit ($x_1\rightarrow x_1+1$), then the odds ratio changes by $e^{\beta_1}$.

## Binary Classifier metrics

### Confusion matrix

The confusion matrix is a 2x2 table summarizing the number of correct predictions of the model (a function of the decision boundary):

|          | predict 1                | predict 0                |
|----------|--------------------------|--------------------------|
| data = 1 | true positives (TP)      | false negatives (FN)[^1] |
| data = 0 | false positives (FP)[^2] | true negatives (TN)      |

[^1]: Type II error

[^2]: Type I error

## Binary Classifier metrics

### Accuracy

Accuracy measure the percent of correct predictions:

$$
\frac{\text{TP}+\text{TN}}{\text{total # observations}}
$$

### Precision

Accuracy measure the percent of positive predictions that are correct:

$$
\frac{\text{TP}}{\text{TP}+\text{FP}}
$$

## Binary Classifier metrics

### Recall / Sensitivity

Measures the success at predicting the first class

$$
\frac{\text{TP}}{\text{TP}+\text{FN}}\qquad\text{(True Positive Rate - TPR)}
$$

### Recall / Specificity

Measures the success at predicting the second class

$$
\frac{\text{TN}}{\text{TN}+\text{FP}}\qquad\text{(True Negative Rate - TNR)}
$$

## Binary Classifier metrics

### ROC Curves

Consider plotting the TPR against the FPR (1-TNR) [**at different classification thresholds**]{.underline}. This is the ROC.

-   the diagonal (TPR = 1-TNR) describes a process equivalent to tossing a fair coin (i.e. no predictive power)
-   our method should have a curve above the diagonal; which shape is better depends on the purpose of our classifier.

So, how to compute?

## Example: create the workflow

#### Workflow to model credit card default

```{r}
#| echo: true
#| code-fold: true
#| fig-align: center
data <- ISLR::Default %>% tibble::as_tibble()
set.seed(8740)

# split data
data_split <- rsample::initial_split(data)
default_train <- rsample::training(data_split)

# create a recipe
default_recipe <- default_train %>% 
  recipes::recipe(formula = default ~ student + balance + income) %>% 
  recipes::step_dummy(recipes::all_nominal_predictors())

# create a linear regression model
default_model <- parsnip::logistic_reg() %>% 
  parsnip::set_engine("glm") %>% 
  parsnip::set_mode("classification")

# create a workflow
default_workflow <- workflows::workflow() %>%
  workflows::add_recipe(default_recipe) %>%
  workflows::add_model(default_model)
```

## Example: fit the model using the data

```{r}
#| echo: true
#| code-fold: false
# fit the model
lm_fit <- 
  default_workflow %>% 
  parsnip::fit(default_train)

# augment the data with the predictions using the model fit
training_results <- 
  broom::augment(lm_fit , default_train) 

training_results %>% dplyr::slice_head(n=6)
```

## Example: compute the AUC

::: columns
::: {.column width="50%"}
```{r}
#| echo: true

training_results %>% 
  yardstick::roc_curve(
    truth = default
    , .pred_No
  ) %>% 
  autoplot() + 
  theme_bw(base_size = 18) 
```
:::

::: {.column width="50%"}
```{r}
#| echo: true
training_results %>% 
   yardstick::roc_auc(
     truth = default
     , .pred_No
    )


```
:::
:::

# Other Classification Methods

## Naive Bayes Classification

This method starts with Bayes rule: for $K$ classes and $N$ features, since $\mathbb{P}\left[\left.C_{k}\right|x_{1},\ldots,x_{N}\right]\times\mathbb{P}\left[x_{1},\ldots,x_{N}\right]$ is equal to $\mathbb{P}\left[\left.x_{1},\ldots,x_{N}\right|C_{k}\right]\times\mathbb{P}\left[C_{k}\right]$, we can write

$$
\mathbb{P}\left[\left.C_{k}\right|x_{1},\ldots,x_{N}\right]=\frac{\mathbb{P}\left[\left.x_{1},\ldots,x_{N}\right|C_{k}\right]\times\mathbb{P}\left[C_{k}\right]}{\mathbb{P}\left[x_{1},\ldots,x_{N}\right]}
$$

## Naive Bayes Classification

If we assume that the features are all independent we can write Bayes rule as

$$
\mathbb{P}\left[\left.C_{k}\right|x_{1},\ldots,x_{N}\right]=\frac{\mathbb{P}\left[C_{k}\right]\times\prod_{n=1}^{N}\mathbb{P}\left[\left.x_{n}\right|C_{k}\right]}{\prod_{n=1}^{N}\mathbb{P}\left[x_{n}\right]}
$$

and our classifier is

$$
C_{k}=\arg\max_{C_{k}}\mathbb{P}\left[C_{k}\right]\prod_{n=1}^{N}\mathbb{P}\left[\left.x_{n}\right|C_{k}\right]
$$

## Naive Bayes Classification

So it remains to calculate the class probability $\mathbb{P}\left[C_{k}\right]$ and the conditional probabilities $\mathbb{P}\left[\left.x_{n}\right|C_{k}\right]$

The different naive Bayes classifiers differ mainly by the assumptions they make regarding the conditional probabilities.

## Naive Bayes Classification

If our features are all ordinal, then

-   The class probabilities are simply the frequency of instances that belong to each class divided by the total number of instances.

-   The conditional probabilities are the frequency of each feature value for a given class value divided by the frequency of instances with that class value.

## Naive Bayes Classification

If any features are numeric, we can estimate conditional probabilities by assuming that the numeric features have a Gaussian distribution for each class

## Naive Bayes Classification

```{r}
#| echo: true
#| code-fold: true
#| fig-align: center
#| message: false
#| warning: false
library(discrim)
# create a naive bayes classifier
default_model_nb <- parsnip::naive_Bayes() %>% 
  parsnip::set_engine("klaR") %>% 
  parsnip::set_mode("classification")

# create a workflow
default_workflow_nb <- workflows::workflow() %>%
  workflows::add_recipe(default_recipe) %>%
  workflows::add_model(default_model_nb)

# fit the model
lm_fit_nb <- 
  default_workflow_nb %>% 
  parsnip::fit(
    default_train
  , control = 
    workflows::control_workflow(parsnip::control_parsnip(verbosity = 1L))
  )

# augment the data with the predictions using the model fit
training_results_nb <- 
  broom::augment(lm_fit_nb , default_train) 
```

## Naive Bayes Classification

::: panel-tabset
## AUC

```{r}
#| echo: true
#| code-fold: false
training_results_nb %>% 
  yardstick::roc_auc(.pred_No, truth = default)
```

## ROC

```{r}
#| echo: false
#| code-fold: true
#| fig-align: center
training_results_nb %>% yardstick::roc_curve(.pred_No, truth = default) %>% autoplot()
```
:::

## Nearest Neighbour Classification

The k-nearest neighbors algorithm, also known as KNN or k-NN, is a non-parametric, supervised learning classifier, which uses proximity to make classifications or predictions about the grouping of an individual data point.

It is typically used as a classification algorithm, working off the assumption that similar points can be found near one another.

## Nearest Neighbour Classification

For classification problems, a class label is assigned on the basis of a majority vote---i.e. the label that is most frequently represented around a given data point is used.

Before a classification can be made, the distance between points must be defined. Euclidean distance is most commonly used.

## Nearest Neighbour Classification

Note that the KNN algorithm is also part of a family of "lazy learning" models, meaning that it only stores a training dataset versus undergoing a training stage. This also means that all the computation occurs when a classification or prediction is being made.

The k value in the k-NN algorithm determines how many neighbors will be checked to determine the classification of a specific query point.

## KNN Classification: distance measures

-   Euclidean: $\text{d}(x,y)=\sqrt{\sum_i(y_i- x_i)^2}$
-   Manhattan: $\text{d}(x,y)=\sum_{i}\left|y_{i}-x_{i}\right|$
-   Minkowski: $\text{d}(x,y)=\left(\sum_{i}\left|y_{i}-x_{i}\right|\right)^{1/p}$

## KNN Classification: algorithm

1.  Choose the value of K, which is the number of nearest neighbors that will be used to make the prediction.
2.  Calculate the distance between the point you want to classify and all the points in the training set.
3.  Select the K nearest neighbors based on the distances calculated.
4.  Assign the label of the majority class to the new data point.
5.  Repeat steps 2 to 4 for all the data points in the test set.

## KNN Classification: model

Our classification workflow only differs by the model, e.g.:

```{r}
#| echo: true
default_model_knn <- parsnip::nearest_neighbor(neighbors = 4) %>% 
  parsnip::set_engine("kknn") %>% 
  parsnip::set_mode("classification")
```

::: callout-note
## k-NN regression

To use k-NN for a regression problem, calculate the mean or median (or another aggregate measure) of the dependent variable among the k neighbors.
:::

## KNN Classification: metrics

In the context of k-Nearest Neighbors (kNN) classification, while the general evaluation metrics like accuracy, precision, recall, F1 score, and others are commonly used, there are no unique metrics that are exclusively specific to kNN. However, there are certain considerations and additional analyses that are particularly relevant when evaluating a kNN model.

## KNN Classification: metrics

::: {style="font-size: smaller"}
1.  **Choice of k (Number of Neighbors):** The value of k affects the performance of a kNN model. Testing the model with various values of k and evaluating the performance using standard metrics (like accuracy, F1 score) can to select the best k.

2.  **Feature Scaling Sensitivity:** kNN is sensitive to the scale of the features because it relies on calculating distances. Evaluate the model's performance before and after feature scaling (like Min-Max scaling or Z-score normalization)

3.  **Curse of Dimensionality:** kNN can perform poorly with high-dimensional data (many features). Evaluating the model's performance in relation to the number of features (dimensionality) can be important. Dimensionality reduction techniques like PCA might be used with kNN.
:::

## Support Vector Machine Classification

The SVM assumes a training set of the form $(x_1,y_1),\ldots,(x_n,y_n)$ where the $y_i$ are either $-1$ or $1$, indicating the class to which each $x_i$ belongs.

The SVM algorithm looks to find the **maximum-margin hyperplane** that divides the group of points $x_i$ for which $y_1=-1$ from the group for which $Y_1=1$, such that the distance between the hyperplane and the nearest point $x_i$ from either group is maximized.

## SVM Classification

### Basic Concepts

::: {style="font-size: smaller"}
1.  **Separating Hyperplane:** The core idea of SVM is to find a hyperplane (in two-dimensional space, this would be a line) that best separates the classes in the feature space.
2.  **Support Vectors:** Support vectors are the data points that are closest to the separating hyperplane. These points are critical in defining the position and orientation of the hyperplane.
3.  **Margin:** The algorithm aims to maximize the margin, which is the distance between the hyperplane and the nearest points from both classes. A larger margin is considered better as it may lead to lower generalization error of the classifier.
:::

## SVM Classification: large margin

Illustration of SVM large-margin principle

[![](images/lec-6/Murphy_Figure_17.12.png){fig-alt="from Murphy figure 17.12" fig-align="center"}](https://github.com/probml/pml-book/blob/main/book1-figures/Figure_17.12.png)

## SVM Classification:

::: columns
::: {.column width="50%" style="font-size: smaller"}
-   Let our decision boundary be given by $f\left(x\right)=w^{\top}x+w_{0}=0$, for a vector $w$ perpendicular to the boundary[^3].

-   We can express any point as $x=x_{\bot}+r\frac{w}{\left\Vert w\right\Vert }$

-   Note that $f\left(x\right)=\left(w^{\top}x_{\bot}+w_{0}\right)+r\left\Vert w\right\Vert$
:::

::: {.column width="50%"}
![](images/lec-6/Murphy_Figure_17.13_A.png){fig-align="center"}
:::
:::

[^3]: any point $x$ on the red line satisfies $w^\top x+w_0=0$

## SVM Classification:

::: columns
::: {.column width="50%" style="font-size: smaller"}
-   Since $f\left(x_{\bot}\right)=w^{\top}x_{\bot}+w_{0}=0$, we have $f\left(x\right)=r\left\Vert w\right\Vert$.

-   We also require $f\left(x_{n}\right)\tilde{y}_{n}>0$

-   To maximize the distance to the closest point, the objective is $\max_{w,w_{0}}\min_{n}\left[\tilde{y}_{n}\left(w^{\top}x_{n}+w_{0}\right)\right]$
:::

::: {.column width="50%"}
![](images/lec-6/Murphy_Figure_17.13_A.png){fig-align="center"}
:::
:::

## SVM Classification:

It is common to scale the vector $w$ and the offset $w_0$ such that $f_n\hat{y}_n=1$ for the point nearest the decision boundary, such that $f_n\hat{y}_n\ge1$ for all $n$.

In addition, since minimizing $1/ \left\Vert w\right\Vert$ is equivalent to minimizing $\left\Vert w\right\Vert^2$, we can state the objective as

$$
\min_{w,w_{0}}\frac{{1}}{2}\left\Vert w\right\Vert ^{2}\quad\text{s.t.}\quad\tilde{y}_{n}\left(w^{\top}x_{n}+w_{0}\right)\ge 1, \forall n
$$

## SVM Classification:

::: columns
::: {.column width="50%" style="font-size: smaller"}
If there is no solution to the objective we can add slack variables $\xi_n\ge0$ to replace the hard constraints that $f_n\hat{y}_n\ge1$ with the soft margin constraints that $f_n\hat{y}_n\ge1-\xi_n$.

The new objective is

$$
\min_{w,w_{0},\xi}\frac{{1}}{2}\left\Vert w\right\Vert ^{2} + C\sum_n \xi_n \\
\text{s.t.}\xi_n\ge0, \quad\tilde{y}_{n}\left(w^{\top}x_{n}+w_{0}\right)\ge 1, \forall n
$$
:::

::: {.column width="50%"}
![](images/lec-6/Murphy_Figure_17.13_B.png){fig-alt="Murphy fig 17.13(b)" fig-align="center" width="299"}
:::
:::

## SVM: data not separable

::: columns
::: {.column width="50%" style="font-size: smaller"}
If the data is not separable:

-   a transformation of data may make them separable

-   an embedding in a higher dimensional space might make them separable
:::

::: {.column width="50%"}
![](images/lec-6/Sebastian_Raschka.png){fig-align="center" width="600"}
:::
:::

## SVM Classification: Support Vectors

-   Support vectors are the data points that lie closest to the decision surface (or hyperplane)

-   They are the data points most difficult to classify

-   They have direct bearing on the optimum location of the decision surface

-   Support vectors are the elements of the training set that would change the position of the dividing hyperplane if

    removed

## SVM Classification: example

```{r}
#| echo: true
#| code-fold: true
#| eval: false
# show_engines("svm_linear")
default_model_svm <- parsnip::svm_linear() %>% 
  parsnip::set_engine("svm_linear") %>% 
  parsnip::set_mode("classification")
```

# Clustering Methods

## Clustering

***Cluster analysis*** refers to algorithms that group similar objects into groups called *clusters*. The endpoint of cluster analysis is a set of clusters*,* where each cluster is distinct from each other cluster, and the objects within each cluster are broadly similar to each other.

The purpose of cluster analysis is to help reveal patterns and structures within a dataset that may provide insights into underlying relationships and associations.

## Clustering Applications

::: {style="font-size: 70%"}
1.  **Market Segmentation:** Cluster analysis is often used in marketing to segment customers into groups based on their buying behavior, demographics, or other characteristics.
2.  **Image Processing:** In image processing, cluster analysis is used to group pixels with similar properties together, allowing for the identification of objects and patterns in images.
3.  **Biology and Medicine:** Cluster analysis is used in biology and medicine to identify genes associated with specific diseases or to group patients with similar clinical characteristics together.
4.  **Social Network Analysis:** In social network analysis, cluster analysis is used to group individuals with similar social connections and characteristics together, allowing for the identification of subgroups within a larger network.
5.  **Anomaly Detection:** Cluster analysis can be used to detect anomalies in data, such as fraudulent financial transactions, unusual patterns in network traffic, or outliers in medical data.
:::

## K-means Clustering

*k-means* is a method of *unsupervised* learning that produces a partitioning of observations into *k* unique clusters.

The goal of *k-means* is to minimize the sum of squared Euclidian distances between observations in a cluster and the **centroid**, or geometric mean, of that cluster.

## K-means Clustering

In *k-means* clustering, observed variables (columns) are considered to be locations on axes in multidimensional space.

The basic k-means algorithm has the following steps.

1.  pick the number of clusters k
2.  Choose *k* observations in the dataset. These locations in space are declared to be the **initial centroids**.
3.  Assign each observation to the nearest **centroid**.
4.  Compute the new **centroids** of each cluster.
5.  Repeat steps 3 and 4 until the **centroids** do not change.

## K-means Clustering

There are three common methods for selecting initial centers:

1.  **Random observations:** Chosing random observations to act as our initial centers is the most commonly used approach, implemented in the `Forgy`, `Lloyd`, and `MacQueen` methods.
2.  **Random partition:** The observations are assigned to a cluster uniformly at random. The centroid of each cluster is computed, and these are used as the initial centers. This approach is implemented in the `Hartigan-Wong` method.
3.  **k-means++:** Beginning with one random set of the observations, further observations are sampled via probability-weighted sampling until $k$ clusters are formed. The centroids of these clusters are used as the initial centers.

## K-means Clustering

Because the initial conditions are based on random selection in both approaches, the k-means algorithm is not deterministic. That is, running the clustering twice on the same data may not result in the same cluster assignments.

## K-means Example

```{r}
#| echo: true
#| code-fold: true
#| eval: false
#| code-line-numbers: "1-3|5-10|11-12|14-22"
# create recipe for 2-D clustering
cluster_recipe <- data %>% 
  recipes::recipe(~ x1 + x2, data = .)

# specify the workflows
all_workflows <- 
  workflowsets::workflow_set(
    preproc = list(base = cluster_recipe),
    models = list(tidyclust::k_means( num_clusters = parsnip::tune() ) )
  )
# create bootstrap samples
dat_resamples <- data %>% rsample::bootstraps(apparent = TRUE)

tuned_results <-
   all_workflows %>% 
   workflow_map(
      fn = "tune_cluster"
      , resamples = dat_resamples
      , grid = dials::grid_regular(dials::num_clusters(), levels = 10)
      , metrics = tidyclust::cluster_metric_set(sse_within_total, sse_total, sse_ratio)
      , control = tune::control_grid(save_pred = TRUE, extract = identity)
   )
```

```{r}
#| echo: false
#| code-fold: true
#| eval: false
set.seed(8740)

centers <- tibble::tibble(
  cluster = factor(1:4), 
  num_points = c(100, 150, 50, 90),  # number points in each cluster
  x1 = c(5, 0, -3, -4),              # x1 coordinate of cluster center
  x2 = c(-1, 1, -2, 1.5)               # x2 coordinate of cluster center
)

labelled_points <- 
  centers %>%
  dplyr::mutate(
    x1 = purrr::map2(num_points, x1, rnorm),
    x2 = purrr::map2(num_points, x2, rnorm)
  ) %>% 
  dplyr::select(-num_points) %>% 
  tidyr::unnest(cols = c(x1, x2))

ggplot(labelled_points, aes(x1, x2, color = cluster)) +
  geom_point(alpha = 0.3)
```

```{r}
#| echo: false
#| code-fold: true
#| eval: false
# penguins <- modeldata::penguins
# 
# labelled_points <- penguins %>%
#   dplyr::select(bill_length_mm, bill_depth_mm) %>%
#   tidyr::drop_na() %>%
#   # shuffle rows
#   dplyr::slice_sample( n = nrow(penguins) )

# create recipe
labelled_points_recipe <- labelled_points %>% 
  recipes::recipe(~ x1 + x2, data = .)


```

```{r}
#| echo: false
#| code-fold: true
#| eval: false
labelled_points
```

```{r}
#| echo: false
#| code-fold: true
#| eval: false
kmeans_spec <- tidyclust::k_means( num_clusters = 3 )
```

```{r}
#| echo: false
#| code-fold: true
#| eval: false
wflow <- workflows::workflow() %>%
  workflows::add_model(kmeans_spec) %>%
  workflows::add_recipe(labelled_points_recipe)

set.seed(8740)
labelled_points_resamples <- labelled_points %>% rsample::bootstraps(apparent = TRUE)
```

```{r}
#| echo: false
#| code-fold: true
#| eval: false
wflow %>%
  parsnip::fit(labelled_points) %>% 
  broom::tidy()
```

```{r}
#| echo: false
#| eval: false
wflow %>%
  parsnip::fit(labelled_points) %>% tidyclust::extract_centroids()
```

```{r}
#| eval: false
# all_workflows <- all_workflows %>% 
#   workflowsets::workflow_map(
#     resamples = train_resamples, grid = 5, verbose = TRUE
#   )
```

```{r}
#| eval: false
# all_workflows <- 
#   workflowsets::workflow_set(
#     preproc = list(base = labelled_points_recipe),
#     models = 
#       3:20 %>% purrr::map( ~tidyclust::k_means( num_clusters = .x ) )
#   )
```

```{r}
#| echo: false
#| eval: false
# all_workflows <- all_workflows %>% 
#   workflowsets::workflow_map(
#     verbose = TRUE                # enable logging
#     , fn = "tune_cluster"
#     , resamples = labelled_points_resamples # a parameter passed to tune::tune_grid()
#   )
```

```{r}
#| echo: false
#| eval: false
# all_workflows %>% 
#   dplyr::select(wflow_id,result) %>% 
#   tidyr::unnest(result) %>% 
#   tidyr::unnest(.metrics) %>% 
#   dplyr::filter(.metric == 'sse_total') %>% 
#   dplyr::group_by(wflow_id) %>% 
#   dplyr::arrange(desc(.estimate) ) %>% 
#   dplyr::slice(1)
```

```{r}
#| eval: false
all_workflows <- 
  workflowsets::workflow_set(
    preproc = list(base = labelled_points_recipe),
    models = list(tidyclust::k_means( num_clusters = parsnip::tune() ) )
  )
```

```{r}
#| echo: false
#| eval: false
all_workflows <- all_workflows %>% 
  workflowsets::workflow_map(
    verbose = TRUE                # enable logging
    , fn = "tune_cluster"
    , resamples = labelled_points_resamples # a parameter passed to tune::tune_grid()
  )
```

```{r}
#| echo: false
#| eval: false
all_workflows %>% workflowsets::rank_results(select_best = TRUE)
```

```{r}
#| echo: false
#| eval: false
all_workflows %>% 
  dplyr::select(wflow_id,result) %>% 
  tidyr::unnest(result) %>% 
  tidyr::unnest(.metrics) %>% 
  dplyr::filter(.metric == 'sse_within_total') %>% 
  dplyr::group_by(wflow_id) %>% 
  dplyr::arrange(desc(.estimate) ) %>% 
  dplyr::slice(1)
```

```{r}
#| echo: false
#| eval: false
tune_results <-
   all_workflows %>% 
   workflow_map(
      fn = "tune_cluster"
      , resamples = labelled_points_resamples
      , grid = dials::grid_regular(dials::num_clusters(), levels = 10)
      , metrics = tidyclust::cluster_metric_set(sse_within_total, sse_total, sse_ratio)
      , control = tune::control_grid(save_pred = TRUE, extract = identity)
   )
```

## Hierarchical Clustering

*Hierarchical Clustering*, sometimes called *Agglomerative Clustering*, is a method of *unsupervised* learning that produces a *dendrogram*, which can be used to partition observations into clusters (see [tidyclust](https://tidyclust.tidymodels.org/articles/hier_clust.html))

For other clustering algorithms, see

::: {style="font-size: 80%"}
-   DBSCAN (Density-Based Spatial Clustering of Applications with Noise)
-   Mean Shift Clustering
-   Spectral Clustering
:::

## Clustering: General Considerations

-   **Feature Scaling:** Most clustering algorithms benefit from feature scaling.

-   **Choosing the Right Algorithm:** Depends on the size, dimensionality of data, and the nature of the clusters.

-   **Evaluation:** Since clustering is unsupervised, evaluating the results can be subjective and is often based on domain knowledge.

## More

-   Read [An Idiot's Guide to Support Vector Machines](https://web.mit.edu/6.034/wwwbob/svm.pdf)

## Recap

-   We have looked at several classification algorithms in the context of tidymodels workflows.

-   We also looked at clustering and several algorithms in the `tidyclust` package.
