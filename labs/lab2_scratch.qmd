---
title: 'Exploratory Analysis for Tate dataset'
date: '`r Sys.Date()`'
output:
  html_document:
    number_sections: true
    fig_caption: true
    toc: true
    fig_width: 7
    fig_height: 4.5
    theme: cosmo
    code_folding: hide
---

This kernel is intended to analyze the main features of Tate dataset. The idea is to conduct some exploratory analysis in order to have a basic understanding of the dataset. I will update the kernel for the next few days, implementing new chunks of code each time I can: the aim is to create a tutorial of possible exploratory analysis for this dataset.

# Exploratory analysis

```{r, message = FALSE}
#| eval: false
require(magrittr)
the_tate <- readr::read_delim("data/the-tate-collection.csv", ";", escape_double = FALSE, trim_ws = TRUE)
the_tate_artists <- readr::read_csv("data/the-tate-artists.csv")
```

## A short summary of the dataset

```{r}
#| eval: false
the_tate %>% DataExplorer::introduce() %>% dplyr::glimpse()
the_tate %>% DataExplorer::plot_missing()
```

## E2: missing year data

```{r}
#| eval: false
missing_year <- the_tate %>% 
  dplyr::filter(is.na(year))
misssing_year

missing_year$artist %>% table() %>% 
  tibble::as_tibble() %>% 
  arrange(desc(n)) %>% 
  mutate(total = sum(n), pct_of_missing = n/total, cum_pct = cumsum(pct_of_missing))
```

## E3

```{r}
#| eval: false
the_tate %>% 
  dplyr::group_by(artist) %>% 
  dplyr::mutate(count = n()) %>% 
  dplyr::arrange(desc(count)) %>% 
  dplyr::select(artist,count) %>% 
  dplyr::distinct()
```

### E ?

```{r}
#| eval: false
n_all <- (the_tate %>% 
  dplyr::select(artist,title) %>% 
  dim())[1]
  

n_distinct <- (the_tate %>% 
  dplyr::select(artist,title) %>% 
  dplyr::distinct() %>% 
  dim())[1]

n_all - n_distinct
```

First of all, let's analyze the entire dataset as it is. We have 69201 observations, each one corresponding to an artwork in Tate collection. For each observation/artwork, we have 20 attributes, including artist, title, date, medium, dimensions and Tate's acquisition year. Some general observations about the dataset:

-   `unique(the_tate$artist)` shows us that there are 3336 different artists in the dataset,

-   `summary(the_tate$year)` illustrates the period of time covered by artworks (1545 - 2012),

-   `summary(the_tate$acquisitionYear)` gives us the period of time in which Tate has acquired artworks (1823 - 2013).

```{r}
#| eval: false
the_tate %>% 
  dplyr::summarize(
    artists = unique(artist) %>% length()
    , min_yr = min(year, na.rm = TRUE), max_yr = max(year, na.rm = TRUE)
    , min_ac = min(acquisitionYear, na.rm = TRUE), max_ac = max(acquisitionYear, na.rm = TRUE)
  )
```

```{r, message = FALSE}
#| eval: false
# the_tate_area <- the_tate[!is.na(the_tate$width) & !is.na(the_tate$height),]
# area <- the_tate_area$width * the_tate_area$height
# the_tate_area <- cbind(the_tate_area, area)
# the_tate_area <- the_tate_area[order(the_tate_area$area),]

the_tate %>% 
  dplyr::mutate(area = width * height / 100) %>% 
  dplyr::select(artist, title, area) %>% 
  dplyr::arrange(desc(area)) %>% 
  tidyr::drop_na() %>% 
  dplyr::slice_head(n=1)

the_tate %>% 
  dplyr::mutate(area = width * height / 100) %>% 
  dplyr::select(artist, title, area) %>% 
  dplyr::arrange(desc(area)) %>% 
  tidyr::drop_na() %>% 
  dplyr::slice_tail(n=1)
```

We can create a variable `area` by multiplying `width` and `height` and then we can order the dataset according to this variable, so that we can detect the smallest and the biggest artwork available at Tate.

-   The smallest one will be the one in first row, `the_tate_area[1,]`, *Thème de Ballet* by *E.L.T. Mesens*.

-   The biggest one will be the one in last row, `the_tate_area[nrow(the_tate_area),]`, *Table and Four Chairs* by *Robert Therrien*.

And what about the artists? If we group the dataset according to artist name (or artist ID) and we count rows, we obtain a table (that we order descending) with the number of artworks for each artist. It is easy to see that more than half of the artworks are by *Joseph Mallord William Turner*.

```{r, message = FALSE}
#| eval: false
the_tate_by_artist <- the_tate %>%
  dplyr::group_by(artist) %>%
  dplyr::summarize(count = n())

the_tate_by_artist %>% 
  dplyr::arrange(desc(count)) %>% 
  dplyr::mutate("pct of collection" = count/sum(count)) %>% 
  dplyr::slice_head(n=10) %>% 
  gt::gt("artist") %>% 
  gt::fmt_percent(columns = `pct of collection`) %>% 
  gtExtras::gt_theme_espn()
```

So, now we know that

-   69201 artworks are available at Tate,

-   Tate's artworks cover the period 1545 - 2012,

-   Tate's acquisitions have begun in 1823,

-   the smallest artwork at Tate is *Thème de Ballet* by *E.L.T. Mesens*,

-   the biggest one is *Table and Four Chairs* by *Robert Therrien*,

-   the most represented artist is *Joseph Mallord William Turner*.

## Titles' text analysis

If we want to have a naive idea of what are the artworks about, we can analyze the titles we the use of a tag cloud. Why are we using tag cloud? First of all, tag clouds are simple and clear: the most used keywords are underlined by the size and the color. Moreover, tag clouds are easy to understand, to be shared and are impactful.

First of all, we have to open the libraries needed: *tm*, *SnowballC* and *wordcloud*.

```{r, message = FALSE}
#| eval: false
library(tm)
library(SnowballC)
library(wordcloud)
```

After that, we need to prepare a useful dataset for the task: in this case we have to remove all the observation that do not have a title. Pay attention to the fact that in this dataset the artworks without a title are not indicated with a `NA`, so we need to take out all the observations that has as title *Untitled*, *Blank* or one of the expression beginning with *\[*. Then we have to transform the vector of titles into an analyzable corpus.

```{r, message = FALSE}
#| eval: false
the_tate_titles <- the_tate[substr(the_tate$title,1,1) != "[" & the_tate$title != "Untitled" & the_tate$title != "Blank",]
titleCorpus <- Corpus(VectorSource(the_tate_titles$title))
```

Now we have to transform the text in order to create the tag cloud. Some consideration are quite easy: we need to remove punctuation, numbers and special characters. Others are not so easy to understand, so I will briefly explain each passage:

-   `tolower` transforms all characters to lower case, this is essential in order to make the following function working properly;

-   `removeWords` remove English stop words, i.e. all common words such as *the*, *and*, *or* and so on;

-   `stemDocument` transforms the words to their roots, so that, for example, *actor*, *actors* and *actres* are reduced to the same word;

-   `stripWhitespace` remove extra spaces created by previous operations.

```{r, message = FALSE}
#| eval: false
# define a function to remove strange characters
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))

titleCorpus <- tm_map(titleCorpus, toSpace, "‘")
titleCorpus <- tm_map(titleCorpus, toSpace, "’")
titleCorpus <- tm_map(titleCorpus, removePunctuation) # remove punctuation
titleCorpus <- tm_map(titleCorpus, tolower) # transform to lower case
titleCorpus <- tm_map(titleCorpus, removeWords, stopwords('english')) # remove common words
titleCorpus <- tm_map(titleCorpus, removeWords, c("the", "this", "etc", "turner")) # remove common words
titleCorpus <- tm_map(titleCorpus, removeNumbers) # remove numbers
titleCorpus <- tm_map(titleCorpus, stemDocument) # transform words to root
titleCorpus <- tm_map(titleCorpus, stripWhitespace) # remove extra white space
```

Finally, it's time to create the tag cloud! We can set the number of words we want to show (in the example, 150) and some other parameters for the appearance of the cloud. As you can see, some the image explains quite clearly the main subjects of the artworks: castles, views, rivers and mountains, mostly.

```{r, message = FALSE}
#| eval: false
wordcloud(titleCorpus, max.words = 150, random.order = FALSE)
```

# Dataset extention

The Tate gives some open data, including a dataset with the basic data about artists. We will join these data with the artworks dataset.

```{r, message = FALSE}
#| eval: false
the_tate_artists <- readr::read_csv("labs/data/the-tate-artists.csv")
the_tate %<>% dplyr::left_join(the_tate_artists, by = c("artistId" = "id"))
summary(the_tate)
```

```{r}
#| eval: false
the_tate %>% tidyr::drop_na(gender, acquisitionYear) %>% 
  dplyr::select(gender, acquisitionYear) %>% 
  dplyr::arrange(gender, acquisitionYear) %>% 
  dplyr::group_by(gender,acquisitionYear) %>% 
  dplyr::mutate(count = n()) %>% 
  dplyr::ungroup() %>% 
  tidyr::pivot_wider(names_from = gender, values_from = count)
```

```{r}
#| eval: false
the_tate %>% tidyr::drop_na(gender, acquisitionYear) %>%
  dplyr::group_by(acquisitionYear, gender) %>%
  dplyr::summarise(n = dplyr::n(), .groups = "drop") %>%
  dplyr::filter(n > 1L)
```

## Artworks acquisition and gender gap

Considering this extended dataset, some more analysis are allowed. First of all, let's analyze gender gap in acquisition: we consider acquisitions through time according to gender (so, first of all, we need to create the two sub-datasets of male and female artists) e we show the trends. We will consider only the XX and XXI centuries.

```{r}
#| eval: false
# the_tate_male \<- the_tate\[the_tate$gender == "Male",] the_tate_female <- the_tate[the_tate$gender == "Female",\]
# 
# count_by_year_male \<- the_tate_male %\>% group_by(acquisitionYear) %\>% summarize(count = n()) count_by_year_male \<- count_by_year_male\[count_by_year_male\$acquisitionYear \>= 1900,\]
# 
# count_by_year_female \<- the_tate_female %\>% group_by(acquisitionYear) %\>% summarize(count = n()) count_by_year_female \<- count_by_year_female\[count_by_year_female\$acquisitionYear \>= 1900,\]
# 
# plot(count_by_year_male$acquisitionYear, count_by_year_male$count, type = "l", col = "blue", main = "Gender gap analysis on acquisitions", xlab = "Years", ylab = "Number of acquired artworks") lines(count_by_year_female$acquisitionYear, count_by_year_female$count, col = "pink")
```

```{r}
#| eval: false
spx_data <- readr::read_csv("data/SPX_HistoricalData_1692322132002.csv", show_col_types = FALSE)

spx_summary <- spx_data %>% dplyr::mutate(
  Date = lubridate::mdy(Date)
  , year = lubridate::year(Date)
  , return = log(`Close/Last`/dplyr::lead(`Close/Last`)) 
  , var = return^2 
  ) %>% 
  dplyr::group_by(year) %>% 
  dplyr::summarize(
    return = sum(return, na.rm = TRUE)
    , volatility = sum(var, na.rm = TRUE) %>% sqrt()
    , .groups = "drop"
  ) 
spx_summary
```

nmmnmnmn

```{r}
#| eval: false
spx_data %>% gtExtras::gt_plt_summary("S&P 500 data")
```

```{r}
#| eval: false
#| echo: true
spx_summary %>% 
  gt::gt('year') %>% 
  gt::fmt_percent(columns = c(return,	volatility), decimals=1, force_sign=TRUE) %>% 
  gt:: grand_summary_rows(
    columns = return
    , fns = list(label="period return", fn="sum") 
    , fmt = ~ gt::fmt_percent(., decimals = 1, force_sign=TRUE)
  ) %>% 
  gt:: grand_summary_rows(
    columns = volatility
    , fns = list(`period volatility` = ~sqrt(sum(.*.)) )
    , fmt = ~ gt::fmt_percent(., decimals = 1, force_sign=TRUE)
  ) %>% 
  gtExtras::gt_theme_espn()
  
```

## E??

```{r}
#| eval: false
the_tate
```

```{r}

```
