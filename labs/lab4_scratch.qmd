---
title: "lab4_scratch"
---

```{r}
#| echo: true
#| eval: true
require(magrittr)
require(ggplot2)
```

```{r}
#| echo: true
#| eval: false
dat_old <- 
  readxl::read_xlsx("data/2023 FE Guide for DOE-release dates before 7-28-2023.xlsx")
```

Read the [data](https://www.fueleconomy.gov/feg/download.shtml) and engineer features

```{r}
#| echo: true
#| warning: false
dat <- 
  readxl::read_xlsx("data/2023 FE Guide for DOE-release dates before 7-28-2023.xlsx")
```

Select the fields and clean up the names (general)

```{r}
#| echo: true
#| eval: false
cars_23 <- dat %>% dplyr::select(
    "Comb FE (Guide) - Conventional Fuel",
    "Eng Displ",'# Cyl',Transmission
    ,"# Gears","Air Aspiration Method Desc"
    ,"Regen Braking Type Desc","Batt Energy Capacity (Amp-hrs)"
    ,"Drive Desc","Fuel Usage Desc - Conventional Fuel","Cyl Deact?", "Var Valve Lift?"
  ) %>% 
  janitor::clean_names()
```

Clean up the names (specific)

```{r}
#| echo: true
#| eval: false
cars_23 %<>% 
  dplyr::rename(
    fuel_economy_combined = comb_fe_guide_conventional_fuel
    , num_cyl = number_cyl
    , num_gears = number_gears
    , air_aspired_method = air_aspiration_method_desc
    , regen_brake = regen_braking_type_desc
    , batt_capacity_ah = batt_energy_capacity_amp_hrs
    , drive = drive_desc
    , fuel_type = fuel_usage_desc_conventional_fuel
    , cyl_deactivate = cyl_deact
    , variable_valve = var_valve_lift
    )
cars_23 %>% dplyr::glimpse()
```

engineer the features

```{r}
#| echo: true
#| eval: false
# fix the int values
cars_23 %<>% 
  dplyr::mutate( 
    dplyr::across(
      .cols = all_of(c('fuel_economy_combined',"num_cyl","num_gears"))
      , .fns = as.integer
    ) 
  ) 
cars_23 %>% DataExplorer::introduce()
cars_23 %>% DataExplorer::plot_missing()
```

```{r}
#| echo: true
#| eval: false
# fix the na values
cars_23 %<>% 
  tidyr::replace_na(
    list(
      batt_capacity_ah = 0
      , regen_brake = ""
    )
  ) 
```

```{r}
#| echo: true
#| eval: false
# create factors
cars_23 %<>% 
  dplyr::mutate( 
    dplyr::across(
      .cols = all_of(
        c( 'transmission','air_aspired_method','regen_brake','drive'
           ,'fuel_type','cyl_deactivate','variable_valve' )
      )
      , .fns = as.factor
    ) 
  ) 
```

Summary

```{r}
#| echo: true
#| eval: false
cars_23 %>% summary()
```

recipe

```{r}
#| echo: true
#| eval: false
cars_23_rec <- cars_23 %>% 
  recipes::recipe(fuel_economy_combined~.) %>% 
  recipes::step_center(recipes::all_numeric()) %>%
  recipes::step_scale(recipes::all_numeric()) %>% 
  recipes::step_dummy(recipes::all_factor())
```

Create grid to test hyper parameters

```{r}
#| echo: true
#| eval: false
#create hyperparameter grid
hyper_grid <- expand.grid(max_depth = seq(3, 6, 1), eta = seq(.2, .35, .01))  
```

train and test

```{r}
#| echo: true
#| eval: false
smp_size <- floor(0.75 * nrow(cars_23))

## set the seed to make your partition reproducible
set.seed(123)
train_ind <- sample(seq_len(nrow(cars_23)), size = smp_size)

train <- cars_23[train_ind, ]
test <- cars_23[-train_ind, ]

cars_23_prep <- cars_23_rec %>% recipes::prep(
    training = train
    , verbose = FALSE
    , retain = TRUE
)

cars_23_train <- cars_23_prep %>% recipes::bake(new_data=train)
cars_23_test  <- cars_23_prep %>% recipes::bake(new_data=test)

```

using loop and 5-fold CV

```{r}
#| echo: true
#| eval: false
xgb_train_rmse <- NULL
xgb_test_rmse  <- NULL

for (j in 1:nrow(hyper_grid)) {
  set.seed(123)
  m_xgb_untuned <- xgboost::xgb.cv(
    data = cars_23_train %>% dplyr::select(-fuel_economy_combined) %>% as.matrix(), 
    label = cars_23_train %>% dplyr::select(fuel_economy_combined) %>% as.matrix(),
    nrounds = 1000,
    objective = "reg:squarederror",
    early_stopping_rounds = 3,
    nfold = 5,
    max_depth = hyper_grid$max_depth[j],
    eta = hyper_grid$eta[j]
    , verbose = FALSE
  )
  
  xgb_train_rmse[j] <- m_xgb_untuned$evaluation_log$train_rmse_mean[m_xgb_untuned$best_iteration]
  xgb_test_rmse[j] <- m_xgb_untuned$evaluation_log$test_rmse_mean[m_xgb_untuned$best_iteration]
  
  # cat(j, "\n")
}    

best <- hyper_grid[which(xgb_test_rmse == min(xgb_test_rmse)),]; best
```

```{r}
#| echo: true
#| eval: false
m_xgb_untuned <- xgboost::xgb.cv(
    data = cars_23_bake %>% as.matrix(), #train[, 2:12],
    label = train[, 1] %>% as.matrix(),
    nrounds = 1000,
    objective = "reg:squarederror",
    early_stopping_rounds = 3,
    nfold = 5,
    max_depth = hyper_grid$max_depth[j],
    eta = hyper_grid$eta[j]
  )

```

```{r}
#| echo: true
#| eval: false
best_xgb <-
  xgboost::xgboost(
    data = cars_23_test %>% dplyr::select(-fuel_economy_combined) %>% as.matrix(), 
    label = cars_23_test %>% dplyr::select(fuel_economy_combined) %>% as.matrix(),
    nrounds = 1000,
    objective = "reg:squarederror",
    early_stopping_rounds = 3,
    max_depth = best[1,1],
    eta = best[1,2]
    , verbose = FALSE
  )   
```

```{r}
#| echo: true
#| eval: false
pred_xgb <- predict(m1_xgb, test[, 2:34])
```

```{r}
#| echo: true
#| eval: false
m1_xgb <-
  xgboost::xgboost(
    data = cars_23_train %>% dplyr::select(-fuel_economy_combined) %>% as.matrix(), 
    label = cars_23_train %>% dplyr::select(fuel_economy_combined) %>% as.matrix(),
    nrounds = 1000,
    objective = "reg:squarederror",
    early_stopping_rounds = 3,
    max_depth = 6,
    eta = .25
    , verbose = FALSE
  )
```

```{r}
#| echo: true
#| eval: false
pred_xgb <- predict(m1_xgb, cars_23_train %>% dplyr::select(-fuel_economy_combined) %>% as.matrix() )

yhat <- pred_xgb
y <- train[, 1] %>% as.matrix()
caret::postResample(yhat, y)
```

```{r}
#| echo: true
#| eval: false
r <- y - yhat
plot(r, ylab = "residuals", main = "extreme gradient boosting")
plot(y,
     yhat,
     xlab = "actual",
     ylab = "predicted",
     main = "extreme gradient boosting")
abline(lm(yhat ~ y))
```

```{r}
#| echo: true
#| eval: false
#plot first 3 trees of model
xgboost::xgb.plot.tree(model = m1_xgb, trees = 0:2)
```

```{r}
#| echo: true
#| eval: false
importance_matrix <- xgboost::xgb.importance(model = m1_xgb)
xgboost::xgb.plot.importance(importance_matrix[1:10,], xlab = "Feature Importance")
```

```{r}
#| echo: true
#| eval: false
pred_xgb <- predict(m1_xgb, cars_23_train %>% dplyr::select(-fuel_economy_combined) %>% as.matrix() )

yhat <- pred_xgb
y <- train[, 1] %>% as.matrix()
caret::postResample(yhat, y)

pred_xgb <- predict(best_xgb, cars_23_test %>% dplyr::select(-fuel_economy_combined) %>% as.matrix() )

yhat <- pred_xgb
y <- test[, 1] %>% as.matrix()
caret::postResample(yhat, y)
```

## Linear Regression

```{r}
#| echo: true
#| eval: false
N <- 500
dat0 <- tibble::tibble(
  x0 = rnorm(500)
  , y0 = x0 + rnorm(500)
  , z0 = 0.45*x0 + 0.77*y0 + rnorm(500)
)

plot(dat0$x0, dat0$y0)
fit0 <- lm(y0 ~ x0, data = dat0); plot(y0 ~ x0, data = dat0); abline(fit0)
dat0 %>% ggplot(aes(x=x0,y=y0)) +
  geom_point() + geom_smooth(method='lm')
  

```

```{r}
#| echo: true
#| eval: false
#| layout-ncol: 2
dat1 <- tibble::tibble(
  z1 = rnorm(500)
  , x1 = z1 + rnorm(500)
  , y1 = 0.5*x1 + z1 + rnorm(500)
)

plot(dat1$x1,dat1$y1)
fit1 <- lm(y1 ~ x1, data = dat1); plot(y1~x1, data = dat1); abline(fit1)
dat1 %>% ggplot(aes(x=x1,y=y1)) +
  geom_point() + geom_smooth(method='lm')
  
```

```{r}
#| echo: true
#| eval: false
N <- 500
set.seed(1966)
dat0 <- tibble::tibble(
  x0 = rnorm(500)
  , y0 = -(x0 + rnorm(500))
  , z0 = 0.45*x0 + 0.77*y0 + rnorm(500)
)

plot(dat0$x0, dat0$y0)
fit0 <- lm(y0 ~ x0, data = dat0); plot(y0 ~ x0, data = dat0); abline(fit0)
dat0 %>% ggplot(aes(x=x0,y=y0)) +
  geom_point() + geom_smooth(method='lm')
```

```{r}
#| echo: true
#| eval: false
#| layout-ncol: 2
dat1 <- tibble::tibble(
  z1 = rnorm(500)
  , x1 = z1 + rnorm(500)
  , y1 = -(0.5*x1 + z1 + rnorm(500))
)

plot(dat1$x1,dat1$y1)
fit1 <- lm(y1 ~ x1, data = dat1); plot(y1~x1, data = dat1); abline(fit1)
dat1 %>% ggplot(aes(x=x1,y=y1)) +
  geom_point() + geom_smooth(method='lm')
```

```{r}
#| echo: true
#| eval: false
N <- 500
set.seed(1966)

dat0 <- tibble::tibble(
  price0 = 10+rnorm(500)
  , demand0 = 30-(price0 + rnorm(500))
  , unobserved0 = 0.45*price0 + 0.77*demand0 + rnorm(500)
)

fit0 <- lm(demand0 ~ price0, data = dat0)
est  <- fit0 %>% broom::tidy() %>% dplyr::pull(estimate)
dat0 %>% ggplot(aes(x=price0,y=demand0)) +
  geom_point() + geom_abline(intercept=est[1], slope=est[2], colour = "red") 

fit0u <- lm(demand0 ~ price0 + unobserved0, data = dat0)
```

```{r}
#| echo: true
#| eval: false
set.seed(1966)
dat1 <- tibble::tibble(
  unobserved1 = rnorm(500)
  , price1 = 10 + unobserved1 + rnorm(500)
  , demand1 = 23 -(0.5*price1 + unobserved1 + rnorm(500))
)

fit1 <- lm(demand1 ~ price1, data = dat1)
dat1 %>% ggplot(aes(x=price1,y=demand1)) +
  geom_point() + geom_smooth(method='lm')
```

## Ex 1

```{r}
#| echo: true
#| eval: false
boston <- ISLR2::Boston
summary(boston)
lm_medv_lstat <- lm(medv ~ lstat, data = boston)
summary(lm_medv_lstat)
```

```{r}
#| echo: true
#| eval: false
boston %>% 
  ggplot(aes(x=lstat, y = medv)) + geom_point()
```

## Ex 2

```{r}
#| echo: true
#| eval: false
tibble::tibble(lstat = c(5, 10, 15)) %>% 
  dplyr::mutate(
    results =
      purrr::map(
        lstat
        , ~stats::predict.lm(lm_medv_lstat, tibble::tibble(lstat=.), interval = "confidence") %>% 
          tibble::as_tibble()
      )
  ) %>% 
  tidyr::unnest(results)

```

```{r}
#| echo: true
#| eval: false
tibble::tibble(lstat = c(5, 10, 15)) %>% broom::augment(
  lm_medv_lstat, newdata = ., interval = "confidence"
)
```

```{r}
#| echo: true
#| eval: false
#| message: false
lm_medv_lstat %>% performance::check_model(check=c("linearity","qq","homogeneity", "outliers") )
```

## Ex 3

```{r}
#| echo: true
#| eval: false
lm_medv_all <- lm(medv ~ ., data = boston)
performance::check_collinearity(lm_medv_all)
```

## Ex 4

```{r}
#| echo: true
#| eval: false
N <- 500
set.seed(1966)

dat0 <- tibble::tibble(
  price0 = 10+rnorm(500)
  , demand0 = 30-(price0 + rnorm(500))
  , unobserved0 = 0.45*price0 + 0.77*demand0 + rnorm(500)
)
```

```{r}
#| echo: true
#| eval: false
fit0 <- lm(demand0 ~ price0, data = dat0)
est0  <- fit0 %>% broom::tidy()
dat0 %>% ggplot(aes(x=price0,y=demand0)) +
  geom_point() + 
  geom_abline(
    data = est0 %>% dplyr::select(1:2) %>% tidyr::pivot_wider(names_from = term, values_from =estimate)
    , aes(intercept = `(Intercept)`, slope = price0)
    , colour = "red"
  )

# dat0_rec <- dat0 %>% dplyr::select(1:2) %>% recipes::recipe(demand0 ~ price0) %>% 
#   recipes::step_normalize(recipes::all_predictors()) %>% 
#   recipes::prep() %>% 
#   recipes::bake(new_data=NULL) %>% 
#   lm(demand0 ~ price0, data = .)
```

```{r}
#| echo: true
#| eval: false
set.seed(1966)

dat1 <- tibble::tibble(
  unobserved1 = rnorm(500)
  , price1 = 10 + unobserved1 + rnorm(500)
  , demand1 = 23 -(0.5*price1 + unobserved1 + rnorm(500))
)
```

```{r}
#| echo: true
#| eval: false
fit1 <- lm(demand1 ~ price1, data = dat1)
est1  <- fit1 %>% broom::tidy()
dat1 %>% ggplot(aes(x=price1,y=demand1)) +
  geom_point() + 
  geom_abline(
    data = est1 %>% dplyr::select(1:2) %>% tidyr::pivot_wider(names_from = term, values_from =estimate)
    , aes(intercept = `(Intercept)`, slope = price1)
    , colour = "red"
  )
```

## Ex5

```{r}
#| echo: true
#| eval: false
lm(demand0 ~ price0 + unobserved0, data = dat0) %>% 
  broom::tidy() %>% 
  dplyr::bind_rows(
    lm(demand1 ~ price1 + unobserved1, data = dat1) %>% 
      broom::tidy()
  )
```

```{r}
#| echo: true
#| eval: false
dat1 %>% ggplot(aes(x=price1,y=demand1)) +
  geom_point() + 
  geom_abline(
    data = est1 %>% dplyr::select(1:2) %>% tidyr::pivot_wider(names_from = term, values_from =estimate)
    , aes(intercept = `(Intercept)`, slope = price1)
    , colour = "red"
  )
```

## Ex6

```{r}
#| echo: true
#| eval: false
dat <- 
  readxl::read_xlsx("data/2023 FE Guide for DOE-release dates before 7-28-2023.xlsx")
```

```{r}
#| echo: true
#| eval: false
cars_23 <- dat %>% dplyr::select(
    "Comb FE (Guide) - Conventional Fuel",
    "Eng Displ",'# Cyl',Transmission
    ,"# Gears","Air Aspiration Method Desc"
    ,"Regen Braking Type Desc","Batt Energy Capacity (Amp-hrs)"
    ,"Drive Desc","Fuel Usage Desc - Conventional Fuel","Cyl Deact?", "Var Valve Lift?"
  ) %>% 
  janitor::clean_names()
```

```{r}
#| echo: true
#| eval: false
cars_23 %>% DataExplorer::introduce()
cars_23 %>% DataExplorer::plot_missing()
```

```{r}
#| echo: true
#| eval: false
cars_23 %>% 
  dplyr::rename(
    fuel_economy_combined = comb_fe_guide_conventional_fuel
    , num_cyl = number_cyl
    , num_gears = number_gears
    , air_aspired_method = air_aspiration_method_desc
    , regen_brake = regen_braking_type_desc
    , batt_capacity_ah = batt_energy_capacity_amp_hrs
    , drive = drive_desc
    , fuel_type = fuel_usage_desc_conventional_fuel
    , cyl_deactivate = cyl_deact
    , variable_valve = var_valve_lift
    )
```

```{r}
#| echo: true
#| eval: false
cars_23 %<>% 
  dplyr::mutate( 
    dplyr::across(
      .cols = all_of(c('comb_fe_guide_conventional_fuel',"number_cyl","number_gears"))
      , .fns = as.integer
    ) 
  ) 

cars_23 %<>% 
  tidyr::replace_na(
    list(
      batt_energy_capacity_amp_hrs = 0
      , regen_braking_type_desc = ""
    )
  ) 
```

```{r}
#| echo: true
#| eval: false
cars_23 %<>% 
  dplyr::mutate( 
    dplyr::across(
      .cols = all_of(
        c( 'transmission','air_aspiration_method_desc','regen_braking_type_desc','drive_desc'
           ,'fuel_usage_desc_conventional_fuel','cyl_deact','var_valve_lift' )
      )
      , .fns = as.factor
    ) 
  ) 
```

```{r}
#| echo: true
#| eval: false
cars_23_rec <- cars_23 %>% 
  recipes::recipe(comb_fe_guide_conventional_fuel~.) %>% 
  recipes::step_center(recipes::all_numeric()) %>%
  recipes::step_scale(recipes::all_numeric()) %>% 
  recipes::step_dummy(recipes::all_factor())
```

## Ex 7

```{r}
#| echo: true
#| eval: false
## set the seed to make your partition reproducible
set.seed(123)

train <- cars_23 %>% tibble::rowid_to_column("ID") %>% dplyr::sample_frac(0.70)
test  <- dplyr::anti_join(cars_23 %>% tibble::rowid_to_column("ID"), XXtrain, by = 'ID')

train %>% dplyr::select(-ID); test %>% dplyr::select(-ID)

cars_23_prep <- cars_23_rec %>% recipes::prep(
    training = train 
    , verbose = FALSE
    , retain = TRUE
)

cars_23_train <- cars_23_prep %>% recipes::bake(new_data=NULL)
cars_23_test  <- cars_23_prep %>% recipes::bake(new_data=test)
```

## Ex 8

```{r}
#| echo: true
#| eval: false
untuned_xgb <-
  xgboost::xgboost(
    data = cars_23_train %>% dplyr::select(-comb_fe_guide_conventional_fuel) %>% as.matrix(), 
    label = cars_23_train %>% dplyr::select(comb_fe_guide_conventional_fuel) %>% as.matrix(),
    nrounds = 1000,
    objective = "reg:squarederror",
    early_stopping_rounds = 3,
    max_depth = 6,
    eta = .25
    , verbose = FALSE
  )
```

```{r}
#| echo: true
#| eval: false
yhat <- predict(
  untuned_xgb
  , cars_23_test %>% 
    dplyr::select(-comb_fe_guide_conventional_fuel) %>% 
    as.matrix() 
)

y <- cars_23_test %>% 
    dplyr::select(comb_fe_guide_conventional_fuel) %>% 
    as.matrix() 
caret::postResample(yhat, y)
```

```{r}
#| echo: true
#| eval: false
#create hyperparameter grid
hyper_grid <- expand.grid(max_depth = seq(3, 6, 1), eta = seq(.2, .35, .01))  

xgb_train_rmse <- NULL
xgb_test_rmse  <- NULL

for (j in 1:nrow(hyper_grid)) {
  set.seed(123)
  m_xgb_untuned <- xgboost::xgb.cv(
    data = cars_23_train %>% dplyr::select(-comb_fe_guide_conventional_fuel) %>% as.matrix(), 
    label = cars_23_train %>% dplyr::select(comb_fe_guide_conventional_fuel) %>% as.matrix(),
    nrounds = 1000,
    objective = "reg:squarederror",
    early_stopping_rounds = 3,
    nfold = 5,
    max_depth = hyper_grid$max_depth[j],
    eta = hyper_grid$eta[j],
    verbose = FALSE
  )
  
  xgb_train_rmse[j] <- m_xgb_untuned$evaluation_log$train_rmse_mean[m_xgb_untuned$best_iteration]
  xgb_test_rmse[j] <- m_xgb_untuned$evaluation_log$test_rmse_mean[m_xgb_untuned$best_iteration]
}    

best <- hyper_grid[which(xgb_test_rmse == min(xgb_test_rmse)),]; best
```

```{r}
#| echo: true
#| eval: false
tuned_xgb <-
  xgboost::xgboost(
    data = cars_23_test %>% dplyr::select(-comb_fe_guide_conventional_fuel) %>% as.matrix(), 
    label = cars_23_test %>% dplyr::select(comb_fe_guide_conventional_fuel) %>% as.matrix(),
    nrounds = 1000,
    objective = "reg:squarederror",
    early_stopping_rounds = 3,
    max_depth = best[1,1],
    eta = best[1,2]
    , verbose = FALSE
  ) 
```

```{r}
#| echo: true
#| eval: false
yhat <- predict(
  tuned_xgb
  , cars_23_test %>% 
    dplyr::select(-comb_fe_guide_conventional_fuel) %>% 
    as.matrix() 
)

y <- cars_23_test %>% 
    dplyr::select(comb_fe_guide_conventional_fuel) %>% 
    as.matrix() 
caret::postResample(yhat, y)
```

## Ex 10

```{r}
#| echo: true
#| eval: false
importance_matrix <- xgboost::xgb.importance(model = tuned_xgb)
xgboost::xgb.plot.importance(importance_matrix[1:10,], xlab = "Feature Importance")
```
