---
title: "Principal Component Analysis (PCA) and Component Selection"
---

## PCA (short version)

::: callout-note
The PCA material here is taken from [Probabilistic View of Principal Component Analysis](https://towardsdatascience.com/probabilistic-view-of-principal-component-analysis-9c1bbb3f167)

We'll assume all columns in our data have been normalized - with zero mean and unit standard deviation.
:::

#### SVD

One of the major important concepts in Linear Algebra is SVD and it’s a factorization technique for real or complex matrices where for example a matrix (say *A*) can be factorized as:

$$
A = U\Sigma V^\top 
$$ {#eq-SVD} where $U$,$V^\top$ are orthogonal matrices (transpose equals the inverse) and $\Sigma$ would be a diagonal matrix. A need not be a square matrix, say it’s a $N\times D$ matrix so we can already think of this as our data matrix with $N$ instances and $D$ features. $U,V$ are square matrices ($N\times N$) and ($D\times D$) respectively, and $\Sigma$ will then be an $N\times D$ matrix where the $D\times D$ subset will be diagonal and the remaining entries will be zero.

#### Eigenvalue decomposition

We also know Eigenvalue decomposition. Given a square matrix ($B$) which is diagonalizable can be factorized as:

$$
B = Q\Lambda Q^\top 
$$ {#eq-eigen}

where $Q$ is the square $N\times N$ matrix whose $i$th column is the eigenvector $q_i$ of $B$, and $\Lambda$ is the diagonal matrix whose diagonal elements are the corresponding eigenvalues.

Let’s try to modify equation (@eq-SVD) by multiplying it by $A^\top$.

$$
A^\top A = \left(V\Sigma V^\top\right)\left(U\Sigma^\top V^\top\right)=V\left(\Sigma\Sigma^\top \right)V^\top
$$ Here, $A^\top A$ would be a Square matrix even though $A$ initially didn’t need to be (could be $m\times n$). $\Sigma\Sigma^\top$ is a diagonal matrix and $V$ is an orthogonal matrix. Now, this is basically the eigendecomposition of a matrix $A^\top A$. The eigenvalues here are squares of the singular values for $A$ in eq. (@eq-SVD).

For a positive semi-definite matrix SVD and eigendecomposition are equivalent. PCA boils down to the [**eigendecomposition of the covariance matrix**]{.underline}. Finding the maximum eigenvalue(s) and corresponding eigenvector(s) are basically then can be thought of as finding the direction of maximum variance.

If we have a lot of data (many rows or many columns or both), we'll have a large covariance matrix and large number of eigenvalues and their corresponding eigenvectors (though there can be duplicates).

Do we need them all? How many are just due to noise or measurement error?

#### Random Matrices

Let's perform an experiment, generating a large random data set

```{r}
#| label: normal random symmetric matrix 
#| echo: true
require(ggplot2)

# 5000 rows and columns
n <- 5000
# generate n^2 samples from N(0,1)
m <- array( rnorm(n^2) ,c(n,n))
# make it symmetric
m2 <- (m + t(m))/sqrt(2*n) # t(m) %*% m
# compute eigenvalues and vectors
lambda <- eigen(m2, symmetric=T, only.values = T)

# plot the eignevalues
tibble::tibble(lambda = lambda$values) |> 
  ggplot(aes(x = lambda, y = after_stat(density))) + 
  geom_histogram(color = "white", fill="lightblue", bins=100) + 
  labs(x = 'eignevalues', title = 'Normal random symmetric matrix') +
  theme_minimal()
```

```{r}
# 5000 rows and columns
n <- 5000
# generate n^2 samples from N(0,1)
m <- array( runif(n^2) ,c(n,n))
# make it symmetric
m2 <- sqrt(12)*(m + t(m) -1)/sqrt(2*n) # t(m) %*% m
# compute eigenvalues and vectors
lambda <- eigen(m2, symmetric=T, only.values = T)

# plot the eignevalues
tibble::tibble(lambda = lambda$values) |> 
  ggplot(aes(x = lambda, y = after_stat(density))) + 
  geom_histogram(color = "white", fill="lightblue", bins=100) + 
  labs(x = 'eignevalues', title = 'Uniform random symmetric matrix') +
  theme_minimal()
```

#### Wigner’s semicircle law

Let $\tilde{A}$ be an $N\times N$ matrix with entries $\tilde{A}_{i,j}\sim\mathcal{N\left(0,\sigma^2\right)}$. Define

$$
A_N=\frac{1}{\sqrt{N}}\left(\frac{A+A^\top}{2}\right)
$$ then $A_N$ is symmetric with variance

$$
\mathrm{Var}\left[a_{i,j}\right]=\left\{ \begin{array}{cc}
\sigma^{2}/N & \mathrm{if}\,i\ne j\\
\sigma^{2}/N & \mathrm{if}\,i=j
\end{array}\right.
$$ and the density of the eigenvalues of $A_N$ is given by

$$
\rho_N\left(\lambda\right)\equiv\frac{1}{N}\sum_{i=1}^N\delta\left(\lambda-\lambda_j\right)
$$ which, as shown by Wigner, as

$$
n\rightarrow\infty\rightarrow\begin{array}{cc}
\frac{1}{2\pi\sigma^{2}}\sqrt{4\sigma^{2}-\alpha^{2}} & \mathrm{if}\,\left|\lambda\right|\le2\sigma\\
0 & \mathrm{otherwise}
\end{array}
$$

#### Random correlation matrices

We have $M$ variables with $T$ rows. The elements of the $M\times M$ empirical correlation matrix $E$ are given by:

$$
E_{i,j}=\frac{1}{T}\sum_{t=1}^Tx_{i,j}x_{j,i}
$$ where $x_{i,j}$ denotes the $j$-th (normalized) value of variable $i$. This can be written as $E=H^\top H$ where $H$ is the $T\times M$ dataset.

Assuming the values of $H$ are random with variance $\sigma^2$ then in the limit $T,M\rightarrow\infty$, while keeping the ratio $Q\equiv\frac{T}{M}\ge1$ constant, the density of eigenvalues of $E$ is given by

$$
\rho\left(\lambda\right) = \frac{Q}{2\pi\sigma^2}\frac{\sqrt{\left(\lambda_+-\lambda\right)\left(\lambda_--\lambda\right)}}{\lambda}
$$ where the minimum and maximum eigenvalues are given by

$$
\lambda_\pm=\sigma^2\left(1\pm\sqrt{\frac{1}{Q}}\right)^2
$$

is also known as the **Marchenko-Pastur** distribution that describes the asymptotic behavior of eigenvalues of large random correlation matrices.

```{r}
#| label: IID random normal returns
#| warnings: false
#| errors: false

t <- 5000;
m <- 1000;
h = array(rnorm(m*t),c(m,t)); # Time series in rows
e = h %*% t(h)/t; # Form the correlation matrix
lambdae = eigen(e, symmetric=T, only.values = T);
# ee = lambdae$values;
# hist(ee, breaks =seq(0.01,3.01,.02), main=NA,xlab=”Eigenvalues”, freq=F)

# plot the eignevalues
tibble::tibble(lambda = lambdae$values) |> 
  ggplot(aes(x = lambda, y = after_stat(density))) + 
  geom_histogram(color = "white", fill="lightblue", bins=100) + 
  geom_density() +
  labs(x = 'eignevalues', title = 'Normal random symmetric matrix') +
  xlim(0,3) +
  theme_minimal()

```
