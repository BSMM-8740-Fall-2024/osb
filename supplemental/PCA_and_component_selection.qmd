---
title: "Principal Component Analysis (PCA) and Component Selection"
---

## PCA (short version)

::: callout-note
The PCA material here is taken from [Probabilistic View of Principal Component Analysis](https://towardsdatascience.com/probabilistic-view-of-principal-component-analysis-9c1bbb3f167)

We'll assume all columns in our data have been normalized - with zero mean and unit standard deviation.
:::

#### SVD

One of the major important concepts in Linear Algebra is SVD and it’s a factorization technique for real or complex matrices where for example a matrix (say *A*) can be factorized as:

$$
A = U\Sigma V^\top 
$$ {#eq-SVD} where $U$,$V^\top$ are orthogonal matrices (transpose equals the inverse) and $\Sigma$ would be a diagonal matrix. A need not be a square matrix, say it’s a $N\times D$ matrix so we can already think of this as our data matrix with $N$ instances and $D$ features. $U,V$ are square matrices ($N\times N$) and ($D\times D$) respectively, and $\Sigma$ will then be an $N\times D$ matrix where the $D\times D$ subset will be diagonal and the remaining entries will be zero.

#### Eigenvalue decomposition

We also know Eigenvalue decomposition. Given a square matrix ($B$) which is diagonalizable can be factorized as:

$$
B = Q\Lambda Q^\top 
$$ {#eq-eigen}

where $Q$ is the square $N\times N$ matrix whose $i$th column is the eigenvector $q_i$ of $B$, and $\Lambda$ is the diagonal matrix whose diagonal elements are the corresponding eigenvalues.

Let’s try to modify equation (@eq-SVD) by multiplying it by $A^\top$.

$$
A^\top A = \left(V\Sigma V^\top\right)\left(U\Sigma^\top V^\top\right)=V\left(\Sigma\Sigma^\top \right)V^\top
$$ Here, $A^\top A$ would be a Square matrix even though $A$ initially didn’t need to be (could be $m\times n$). $\Sigma\Sigma^\top$ is a diagonal matrix and $V$ is an orthogonal matrix. Now, this is basically the eigendecomposition of a matrix $A^\top A$. The eigenvalues here are squares of the singular values for $A$ in eq. (@eq-SVD).

For a positive semi-definite matrix SVD and eigendecomposition are equivalent. PCA boils down to the [**eigendecomposition of the covariance matrix**]{.underline}. Finding the maximum eigenvalue(s) and corresponding eigenvector(s) are basically then can be thought of as finding the direction of maximum variance.

If we have a lot of data (many rows or many columns or both), we'll have a large covariance matrix and large number of eigenvalues and their corresponding eigenvectors (though there can be duplicates).

Do we need them all? How many are just due to noise or measurement error?

#### Random Matrices

Let's perform an experiment, generating a large random $N\times N$ data set using $N(0,1)$ measurements.

```{r}
#| label: random symmetric matrix, normal
#| echo: true
#| code-fold: true
#| code-summary: eigenvalues from random symmetric matrix (Normally distributed measurements)

# 5000 rows and columns
n <- 5000
# generate n^2 samples from N(0,1)
m <- array( rnorm(n^2) ,c(n,n))
# make it symmetric
m2 <- (m + t(m))/sqrt(2*n) # t(m) %*% m
# compute eigenvalues and vectors
lambda <- eigen(m2, symmetric=T, only.values = T)

# plot the eignevalues
tibble::tibble(lambda = lambda$values) |> 
  ggplot(aes(x = lambda, y = after_stat(density))) + 
  geom_histogram(color = "white", fill="lightblue", bins=100) + 
  labs(x = 'eignevalues', title = 'Normal random symmetric matrix') +
  theme_minimal()
```

Let's do the same, but with uniform $U(0,1)$ distributed measurements.

```{r}
#| label: random symmetric matrix, uniform
#| code-fold: true
#| code-summary: eigenvalues from random symmetric matrix (Uniformly distributed measurements)
# 5000 rows and columns
n <- 5000
# generate n^2 samples from U(0,1)
m <- array( runif(n^2) ,c(n,n))
# make it symmetric
m2 <- sqrt(12)*(m + t(m) -1)/sqrt(2*n) # t(m) %*% m
# compute eigenvalues and vectors
lambda <- eigen(m2, symmetric=T, only.values = T)

# plot the eignevalues
tibble::tibble(lambda = lambda$values) |> 
  ggplot(aes(x = lambda, y = after_stat(density))) + 
  geom_histogram(color = "white", fill="lightblue", bins=100) + 
  labs(x = 'eignevalues', title = 'Uniform random symmetric matrix') +
  theme_minimal()
```

Note the striking pattern: the density of eigenvalues is a semicircle.

#### Wigner’s semicircle law

Let $\tilde{A}$ be an $N\times N$ matrix with entries $\tilde{A}_{i,j}\sim\mathcal{N\left(0,\sigma^2\right)}$. Define

$$
A_N=\frac{1}{\sqrt{N}}\left(\frac{A+A^\top}{2}\right)
$$ then $A_N$ is symmetric with variance

$$
\mathrm{Var}\left[a_{i,j}\right]=\left\{ \begin{array}{cc}
\sigma^{2}/N & \mathrm{if}\,i\ne j\\
\sigma^{2}/N & \mathrm{if}\,i=j
\end{array}\right.
$$ and the density of the eigenvalues of $A_N$ is given by

$$
\rho_N\left(\lambda\right)\equiv\frac{1}{N}\sum_{i=1}^N\delta\left(\lambda-\lambda_j\right)
$$ which, as shown by Wigner, as

$$
n\rightarrow\infty\rightarrow\begin{array}{cc}
\frac{1}{2\pi\sigma^{2}}\sqrt{4\sigma^{2}-\alpha^{2}} & \mathrm{if}\,\left|\lambda\right|\le2\sigma\\
0 & \mathrm{otherwise}
\end{array}
$$

#### Random correlation matrices

We have $M$ variables with $T$ rows. The elements of the $M\times M$ empirical correlation matrix $E$ are given by:

$$
E_{i,j}=\frac{1}{T}\sum_{t=1}^Tx_{i,j}x_{j,i}
$$ where $x_{i,j}$ denotes the $j$-th (normalized) value of variable $i$. This can be written as $E=H^\top H$ where $H$ is the $T\times M$ dataset.

Assuming the values of $H$ are random with variance $\sigma^2$ then in the limit $T,M\rightarrow\infty$, while keeping the ratio $Q\equiv\frac{T}{M}\ge1$ constant, the density of eigenvalues of $E$ is given by

$$
\rho\left(\lambda\right) = \frac{Q}{2\pi\sigma^2}\frac{\sqrt{\left(\lambda_+-\lambda\right)\left(\lambda-\lambda_-\right)}}{\lambda}
$$ where the minimum and maximum eigenvalues are given by

$$
\lambda_\pm=\sigma^2\left(1\pm\sqrt{\frac{1}{Q}}\right)^2
$$

is also known as the **Marchenko-Pastur** distribution that describes the asymptotic behavior of eigenvalues of large random correlation matrices.

```{r}
#| label: Marchenko-Pastur distribution
#| code-fold: true
#| code-summary: code for Marchenko-Pastur distribution
mpd <- function(lambda,T,M,sigma=1){
  Q <- T/M
  lambda_plus  <- (1+sqrt(1/Q))^2 * sigma^2
  lambda_minus <- (1-sqrt(1/Q))^2 * sigma^2
  if(lambda < lambda_minus | lambda > lambda_plus){
    0
  }else{
    (Q/(2*pi*sigma^2)) * sqrt((lambda_plus-lambda)*(lambda-lambda_minus)) / lambda
  }
}
```

::: panel-tabset
## M = 1000, T = 5000

```{r}
#| code-fold: true
#| warning: false
#| error: false
t <- 5000;
m <- 1000;
h = array(rnorm(m*t),c(m,t)); # Time series in rows
e = h %*% t(h)/t; # Form the correlation matrix
lambdae = eigen(e, symmetric=T, only.values = T);

# create the mp distribution
mpd_tbl <- tibble::tibble(lambda = c(lambdae$values, seq(0,3,0.1)) ) |> 
  dplyr::mutate(mp_dist = purrr::map_dbl(lambda, ~mpd(lambda = ., t,m)))

# plot the eigenvalues
tibble::tibble(lambda = lambdae$values) |> 
  dplyr::mutate(mp_dist = purrr::map_dbl(lambda, ~mpd(lambda = ., t,m))) |> 
  ggplot(aes(x = lambda, y = after_stat(density))) + 
  geom_histogram(color = "white", fill="lightblue", bins=100) + 
  geom_line(data = mpd_tbl, aes(y=mp_dist)) +
  labs(x = 'eigenvalues', title = 'Empirical density'
  , subtitle = 
    stringr::str_glue("with superimposed Marchenko-Pastur density | M={t}, T={m}")
  ) +
  xlim(0,3) +
  theme_minimal()
```

## M = 100, T = 500

```{r}
#| code-fold: true
#| warning: false
#| error: false
t <- 500;
m <- 100;
h = array(rnorm(m*t),c(m,t)); # Time series in rows
e = h %*% t(h)/t; # Form the correlation matrix
lambdae = eigen(e, symmetric=T, only.values = T);

# create the mp distribution
mpd_tbl <- tibble::tibble(lambda = c(lambdae$values, seq(0,3,0.1)) ) |> 
  dplyr::mutate(mp_dist = purrr::map_dbl(lambda, ~mpd(lambda = ., t,m)))

# plot the eigenvalues
tibble::tibble(lambda = lambdae$values) |> 
  dplyr::mutate(mp_dist = purrr::map_dbl(lambda, ~mpd(lambda = ., t,m))) |> 
  ggplot(aes(x = lambda, y = after_stat(density))) + 
  geom_histogram(color = "white", fill="lightblue", bins=30) + 
  geom_line(data = mpd_tbl, aes(y=mp_dist)) +
  labs(x = 'eigenvalues', title = 'Empirical density'
  , subtitle = 
    stringr::str_glue("with superimposed Marchenko-Pastur density | M={t}, T={m}")
  ) +
  xlim(0,3) +
  theme_minimal()
```

## M = 10, T = 50

```{r}
#| warning: false
#| error: false
#| code-fold: true
t <- 50;
m <- 10;
h = array(rnorm(m*t),c(m,t)); # Time series in rows
e = h %*% t(h)/t; # Form the correlation matrix
lambdae = eigen(e, symmetric=T, only.values = T);

# create the mp distribution
mpd_tbl <- tibble::tibble(lambda = c(lambdae$values, seq(0,3,0.1)) ) |> 
  dplyr::mutate(mp_dist = purrr::map_dbl(lambda, ~mpd(lambda = ., t,m)))

# plot the eigenvalues
tibble::tibble(lambda = lambdae$values) |> 
  dplyr::mutate(mp_dist = purrr::map_dbl(lambda, ~mpd(lambda = ., t,m))) |> 
  ggplot(aes(x = lambda, y = after_stat(density))) + 
  geom_histogram(color = "white", fill="lightblue", bins=15) + 
  geom_line(data = mpd_tbl, aes(y=mp_dist)) +
  labs(x = 'eigenvalues', title = 'Empirical density'
  , subtitle = 
    stringr::str_glue("with superimposed Marchenko-Pastur density | M={t}, T={m}")
  ) +
  xlim(0,3) +
  theme_minimal()
```
:::

## Application to correlation matrices

For the special case of correlation matrices (e.g. PCA), we know that $\sigma^2=1$ and $Q = M/T$. This bounds the probability mass over the interval defined by $\left(1\pm\sqrt{\frac{1}{Q}}\right)^2$.

Since this distribution describes the spectrum of random matrices with mean 0, the eigenvalues of correlation matrices (read PCA component weights) that fall inside of the aforementioned interval could be considered spurious or noise. For instance, obtaining a correlation matrix of 10 variables with 252 observations would render

$$
\lambda_+=\left(1\pm\sqrt{\frac{1}{Q}}\right)^2\approx1.43
$$

Thus, out of 10 eigenvalues/components of said correlation matrix, only the values higher than 1.43 would be considered significantly different from random.
