---
output: Causal via Prediction
---

# Where ML Fits into Causal Inference (review)

The traditional go-to tool for causal inference is multiple regression: $$
Y_i = \delta D_i + X_i'\beta+\varepsilon_i,
$$ where $D_i$ is the "treatment" or causal variable whose effects we are interested in, and $X_i$ is a vector of controls, conditional on which we are willing to assume $D_i$ is as good as randomly assigned.

> *example:* Suppose we are interested in the magnitude of racial discrimination in the labor market. One way to conceptualize this is the difference in earnings between two workers who are identical in productivity, but differ in their race, or, the "effect" of race. Then $D_i$ would be an indicator for, say, a Black worker. $Y_i$ would be earnings, and $X_i$ would be characteristics that capture determinants of productivity, including educational attainment, cognitive ability, and other background characteristics.

Where does machine learning fit into causal inference? It might be tempting to treat this regression as a prediction exercise where we are predicting $Y_{i}$ given $D_{i}$ and $X_{i}$. Don't give in to this temptation. We are not after a prediction for $Y_{i}$, we are after a coefficient on $D_{i}$. Modern machine learning algorithms are finely tuned for producing predictions, but along the way they compromise coefficients. So how can we deploy machine learning in the service of estimating the causal coefficient $\delta$?

To see where ML fits in, first remember that an equivalent way to estimate $\delta$ is the following three-step procedure:

1.  Regress $Y_{i}$ on $X_{i}$ and compute the residuals, $\tilde{Y}% _{i}=Y_{i}-\hat{Y}_{i}^{OLS}$, where $\hat{Y}_{i}^{OLS}=X_{i}^{\prime }\left( X^{\prime }X\right) ^{-1}X^{\prime }Y$

2.  Regress $D_{i}$ on $X_{i}$ and compute the residuals, $\tilde{D}% _{i}=D_{i}-\hat{D}_{i}^{OLS}$, where $\hat{D}_{i}^{OLS}=X_{i}^{\prime }\left( X^{\prime }X\right) ^{-1}X^{\prime }D$

3.  Regress $\tilde{Y}_{i}$ on $\tilde{D}_{i}$.

Steps 1 and 2 are prediction exercises--ML's wheelhouse. When OLS isn't the right tool for the job, we can replace OLS in those steps with machine learning:

1.  Predict $Y_{i}$ based on $X_{i}$ using ML and compute the residuals, $\tilde{Y}% _{i}=Y_{i}-\hat{Y}_{i}^{ML}$, where $\hat{Y}_{i}^{ML}$ is the prediction from an ML algorithm

2.  Predict $D_{i}$ based on $X_{i}$ using ML and compute the residuals, $\tilde{D}% _{i}=D_{i}-\hat{D}_{i}^{ML}$, where $\hat{D}_{i}^{ML}$ is the prediction from an ML algorithm

3.  Regress $\tilde{Y}_{i}$ on $\tilde{D}_{i}$.

This is the basis for the two major methods we'll look at today: The first is "Post-Double Selection Lasso" (Belloni, Chernozhukov, Hansen). The second is "Double-Debiased Machine Learning" (Chernozhukov, Chetverikov, Demirer, Duflo, Hansen, Newey, Robins)

# Post Double Selection Lasso (PDS Lasso)

Try it yourself first

```{r}
#| echo: false
#| message: false
library(tidyverse)
library(fixest)
library(rsample)
library(glmnet)
library(randomForest)
```

See [nlsy97](https://www.bls.gov/nls/notices/2021/nlsy97-data-release.htm) for details n the data.

```{r}
#| echo: true
#| message: false
url_str  <- 
  'https://github.com/Mixtape-Sessions/Machine-Learning/blob/main/Labs/data/'
file_str <- 
  'nlsy97.csv?raw=true'
  
nlsy <- readr::read_csv(
  paste0(url_str, file_str, show_col_types = FALSE)
)
# quick peek at the date
head(nlsy)
```

## Define outcome & regressor of interest

-   y: `lnw_2016`
-   d: `black`

## Simple Regression with no Controls

Regress y on d and print out coefficient

```{r}
#| echo: true
#| message: false
lm(lnw_2016 ~ black, data = nlsy)
```

### ...

Is this the effect we're looking for?

Let's try a regression where we control for a few things: education (linearly), experience (linearly), and cognitive ability (afqt, linearly).

```{r}
#| echo: true
fit <- lm(lnw_2016 ~ black + educ + exp + afqt, data = nlsy)

broom::tidy(fit)
```

### ...

How does it compare to the simple regression?

But who is to say the controls we included are sufficient? We have a whole host (hundred!) of other potential controls, not to mention that perhaps the controls we did put in enter linearly. This is a job for ML!

To prep, let's define a matrix X with all of our potential controls:

```{r}
potential_controls <- setdiff(colnames(nlsy), c("lnw_2016", "black"))
```

## Post Double Selection Lasso

### Step 1: Lasso the outcome on X

Don't forget to standard Xs, or choose the normalize=True option

```{r}
nlsy_rec_y <- nlsy %>% 
  recipes::recipe(lnw_2016 ~ .) %>% 
  recipes::step_rm(black)

nlsy_rec_d <- nlsy %>% 
  recipes::recipe(black ~ .) %>% 
  recipes::step_rm(lnw_2016)
```

```{r}
#| eval: false
# Run cross-validation for y
lasso_y <- 
  glmnet::cv.glmnet(
    x = model.matrix(lnw_2016 ~ ., data = nlsy_bake)
    , y = nlsy_bake$lnw_2016
  )
```

```{r}
#| echo: true
#| message: false
set.seed(8740)

lr_cv   <- rsample::vfold_cv(nlsy)
lr_grid <- tibble::tibble(penalty = 10^seq(-4,1, length.out = 200))

lr_mod  <- parsnip::linear_reg(penalty = tune(), mixture = 1) %>%
  parsnip::set_engine("glmnet") %>% 
  parsnip::set_mode("regression")

lr_wf_y <- workflows::workflow() %>%
  workflows::add_model(lr_mod) %>%
  workflows::add_recipe(nlsy_rec_y)

lr_best_y <- lr_wf_y %>%
  tune::tune_grid(
    resamples = lr_cv,
    grid = lr_grid
  ) %>%
  tune::select_best("rmse")

tidy_keep_y <- lr_wf_y %>% 
  tune::finalize_workflow(lr_best_y) %>%
  parsnip::fit(nlsy) %>%
  workflows::extract_fit_parsnip() %>%
  broom::tidy() %>% 
  dplyr::filter(estimate != 0 & term != '(Intercept)') %>% 
  dplyr::pull(term)
```

```{r}
X = as.matrix(nlsy[, potential_controls])
y = nlsy[["lnw_2016"]]

# Run cross-validation for y
lasso_y <- glmnet::cv.glmnet(x=X, y=y)

lasso_y_coefs = coef(lasso_y, lasso_y$lambda.1se)
lasso_y_coefs = as.matrix(lasso_y_coefs)

keep_y = rownames(lasso_y_coefs)[lasso_y_coefs != 0]
# Don't need intercept
keep_y = setdiff(keep_y, "(Intercept)")

```

### Step 2: Lasso the treatment on d

```{r}
lr_wf_d <- lr_wf_y %>% workflows::update_recipe(nlsy_rec_d)

lr_best_d <- lr_wf_d %>%
  tune::tune_grid(
    resamples = lr_cv,
    grid = lr_grid
  ) %>%
  tune::select_best("rmse")

tidy_keep_d <- lr_wf_d %>% 
  tune::finalize_workflow(lr_best_d) %>%
  parsnip::fit(nlsy) %>%
  workflows::extract_fit_parsnip() %>%
  broom::tidy() %>% 
  dplyr::filter(estimate != 0 & term != '(Intercept)') %>% 
  dplyr::pull(term)
```

```{r}

# Run cross-validation for d
d = nlsy[["black"]]
lasso_d <- cv.glmnet(x=X, y=d)

lasso_d_coefs = coef(lasso_d, lasso_d$lambda.1se)
lasso_d_coefs = as.matrix(lasso_d_coefs)

keep_d = rownames(lasso_d_coefs)[lasso_d_coefs != 0]
# Don't need intercept
keep_d = setdiff(keep_d, "(Intercept)")
```

### Step 3: Form the union of controls

```{r}
tidy_keep <- union(tidy_keep_y, tidy_keep_d)
```

```{r}
keep = union(keep_y, keep_d)
```

### Concatenate treatment with union of controls and regress y on that and print out estimate

```{r}
# Need `` surrounding variables since some variables start with underscore
formula = paste(
  "lnw_2016 ~ black + ", 
  paste0("`", tidy_keep, "`", collapse = " + ")
)
formula = as.formula(formula)

lm(formula, data = nlsy) %>% broom::tidy()

# lr_wf_final <- lr_wf_d %>% workflows::update_formula(as.formula(formula))
```

```{r}
# Need `` surrounding variables since some variables start with underscore
formula = paste(
  "lnw_2016 ~ black + ", 
  paste0("`", keep, "`", collapse = " + ")
)
formula = as.formula(formula)

(fullreg = feols(formula, data = nlsy))
```

## Double-Debiased Machine Learning

For simplicity, we will first do it without sample splitting

### Step 1: Ridge outcome on Xs, get residuals

```{r}
# Run cross-validation for y
ridge_y <- cv.glmnet(x=X, y=y, alpha = 0)

y_hat = predict(ridge_y, ridge_y$lambda.1se, newx = X)
nlsy$y_resid = nlsy$lnw_2016 - as.numeric(y_hat)
```

### Step 2: Ridge treatment on Xs, get residuals

```{r}
# Run cross-validation for y
ridge_d <- cv.glmnet(x=X, y=d, alpha = 0)

d_hat = predict(ridge_d, ridge_d$lambda.1se, newx = X)
nlsy$d_resid = nlsy$black - as.numeric(d_hat)
```

### Step 3: Regress y resids on d resids and print out estimate

```{r}
feols(y_resid ~ d_resid, nlsy)
```

### The real thing: with sample splitting

```{r}
set.seed(5)
N_folds = 5
# 5 folds with equal 
nlsy$fold_id = sample(1:N_folds, size = nrow(nlsy), replace = T)

nlsy$y_resid = 0
nlsy$d_resid = 0

# Loop through each fold, use other 4 folds to estimate
for(i in 1:5) {
  in_training = (nlsy$fold_id != i)
  in_test = (nlsy$fold_id == i)

  # Ridge regression for y using training
  ridge_y = cv.glmnet(
    x=X[in_training,], y=y[in_training], alpha = 0
  )
  # Calculate residuals for testing
  nlsy[in_test, "y_resid"] =
    y[in_test] - predict(ridge_y, newx = X[in_test, ])

  # Ridge regression for d using training
  ridge_d = cv.glmnet(
    x=X[in_training,], y=d[in_training], alpha = 0
  )
  # Calculate residuals for testing
  nlsy[in_test, "d_resid"] = 
    d[in_test] - predict(ridge_d, newx = X[in_test, ])
}

# k-fold cross-validation ensures standard errors are fine
feols(
  y_resid ~ d_resid, data = nlsy, vcov = "hc1"
)

```

## Now do DML using Random Forest!

```{r}
set.seed(5)
N_folds = 5
# 5 folds with equal 
nlsy$fold_id = sample(1:N_folds, size = nrow(nlsy), replace = T)

nlsy$y_resid = 0
nlsy$d_resid = 0

# Loop through each fold, use other 4 folds to estimate
for(i in 1:5) {
  in_training = (nlsy$fold_id != i)
  in_test = (nlsy$fold_id == i)

  # Ridge regression for y using training
  ridge_y = randomForest(
    x = X[in_training,], y = y[in_training]
  )
  # Calculate residuals for testing
  nlsy[in_test, "y_resid"] =
    y[in_test] - predict(ridge_y, newdata = X[in_test, ])

  # Ridge regression for d using training
  ridge_d = randomForest(
    x = X[in_training,], y = d[in_training]
  )
  # Calculate residuals for testing
  nlsy[in_test, "d_resid"] = 
    d[in_test] - predict(ridge_d, newdata = X[in_test, ])
}

# k-fold cross-validation ensures standard errors are fine
feols(
  y_resid ~ d_resid, data = nlsy, vcov = "hc1"
)

```
